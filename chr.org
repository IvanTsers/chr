#+begin_export latex
\section{Implementation}
The package \ty{chr} has hooks for imports, data structures, methods,
and functions.
#+end_export
#+begin_src go <<chr.go>>=
  package chr
  import (
	  //<<Imports>>
  )
  //<<Data structures>>
  //<<Methods>>
  //<<Functions>>
#+end_src
#+begin_export latex
\subsection{\textbf{Data structure \ty{Homologs}}}
!Data structure \ty{Homologs} describes homologous regions of a
!subject found in queries. This data type contains a slice of segments
!of the subject \ty{S} and a map of segregating sites \ty{N}.

The data structure \ty{seg} contains the start \ty{s} and the length
\ty{l} of a segment, as defined in Section~\ref{Data structure seg}.
#+end_export
#+begin_src go <<Data structures>>=
  type Homologs struct {
	  S []seg
	  N map[int]bool
  }
#+end_src
#+begin_export latex
\subsubsection{Method \ty{sort}}
The method \ty{sort} orders \ty{Homologs} by their start positions.
#+end_export
#+begin_src go <<Functions>>=
  func (h *Homologs) sort() *Homologs {
	  sort.Slice(h.S, func(i, j int) bool {
		  return h.S[i].s < h.S[j].s
	  })
	  return h
  }
#+end_src
#+begin_export latex
We import \ty{sort}.
#+end_export
#+begin_src go <<Imports>>=
  "sort"
#+end_src
#+begin_export latex
\subsubsection{Method \ty{filterOverlaps}}
The method \ty{filterOverlaps} removes overlapping homologous regions
to create the longest possible chain of non-overlapping regions.

If slice \ty{Homologs.S} does not contain at least two elements, we
return early. If it does, we sort it and reduce overlapping stacks of
segments to the longest chain of co-linear non-overlapping segments
using the algorithm for two-dimensional fragment
chaining~\cite{ohl13:alg} as implemented in
\ty{phylonium}~\cite{kloe20:alg}. We initialize variables, then we
calculate chain scores, find links of the longest chain through
backtracking, and return the chain.
#+end_export
#+begin_src go <<Methods>>=
  func (h *Homologs) filterOverlaps() {
	  slen := len(h.S)
	  if slen < 2 {
		  return
	  }
	  h.sort()
	  segs := h.S
	  //<<Initialize chaining>>
	  //<<Calculate chain score>>
	  //<<Backtrack the chain>>
	  //<<Return the chain>>
  }
#+end_src
#+begin_export latex
We declare variables describing the chain. For each element of
\ty{segs}, we initialize:
\begin{itemize}
  \itemsep0em
  \item \ty{predecessor}---the previous segment in the longest chain
    ending before \ty{segs[i]});
  \item \ty{score}---the length of the longest chain ending at
    \ty{segs[i]}. The initial score of the chain is the first
    segment's length;
  \item \ty{visited}---whether \ty{segs[i]} has been visited during
    backtracking of chain links.
\end{itemize}
We also initialize the first elements for \ty{score} and
\ty{predecessor}.
#+end_export
#+begin_src go <<Initialize chaining>>=
  predecessor := make([]int, slen)
  score := make([]int, slen)
  visited := make([]bool, slen)
  score[0] = segs[0].l
  predecessor[0] = -1
#+end_src
#+begin_export latex
We calculate the chain score to maximize the number of non-overlapping
segments in \ty{segs}. Starting from the second segment, we traverse
each segment. For each \ty{segs[i]}, we find its preceding segment
\ty{segs[k]} that can form the longest chain ending before
\ty{segs[i]} starts.
#+end_export
#+begin_src go <<Calculate chain score>>=
  for i := 1; i < slen; i++ {
	  maxScore := 0
	  maxIndex := -1
	  for k := 0; k < i; k++ {
		  if segs[k].end() <= segs[i].s {
			  if score[k] > maxScore {
				  maxScore = score[k]
				  maxIndex = k
			  }
		  }
	  }
	  predecessor[i] = maxIndex
	  if maxIndex != -1 {
	      score[i] = segs[i].l + score[maxIndex]
	  } else {
	      score[i] = segs[i].l
	  }
  }
#+end_src
#+begin_export latex
We are to find \ty{s}, which is the index of the highest-scoring
segment in \texttt{segs}. This segment is the final link in the chain
we're looking for, and its score is the total score of the chain. To
find \ty{s}, we will use the \ty{argmax} function, which we will
define shortly. Once we identify \ty{s}, we go through the chain of
\ty{predecessor} links and mark the visited segments in \ty{visited}.
#+end_export
#+begin_src go <<Backtrack the chain>>=
  s := argmax(score)
  for s != -1 {
	  visited[s] = true
	  s = predecessor[s]
  }
#+end_src
#+begin_export latex
The function \ty{argmax} returns the index of the maximum value in the
input slice of integers.
#+end_export
#+begin_src go <<Functions>>=
  func argmax(x []int) int {
      maxIdx := 0
      for i := 1; i < len(x); i++ {
	  if x[i] > x[maxIdx] {
	      maxIdx = i
	  }
      }
      return maxIdx
  }
#+end_src
#+begin_export latex
We extract only visited elements from \ty{segs} and set the new value
of \ty{h.S}. We also reset the segregating site map if the related
segment was not visited to avoid reporting phantom mutations. We
finish this block with updating the fields of the homolog \ty{h}.
#+end_export
#+begin_src go <<Return the chain>>=
  var segred []seg
  for i := 0; i < slen; i++ {
	  if visited[i] {
		  segred = append(segred, segs[i])
	  }
  }
  h.S = segred
#+end_src
#+begin_export latex
\subsection{Data structure \ty{seg}} \label{Data structure seg}
We declare a custom data struct \ty{seg} to store segments of the
forward strand of the subject. A \ty{seg} contains a zero-based start
of a region of a subject sequence, its length, and a map \ty{n} to
store coordinates of segregating sites (on the subject) related to
this segment.
#+end_export
#+begin_src go <<Data structures>>=
  type seg struct {
	  s int
	  l int
	  n map[int]bool
  }
#+end_src
#+begin_export latex
\subsubsection{Method \ty{end}}
The method \ty{end()} returns an exclusive coordinate of the end of a
\ty{seg}. Thus, the package operates with zero-based, end-exclusive
coordinates.
#+end_export
#+begin_src go <<Methods>>=
  func (seg *seg) end() int {
	  return seg.s + seg.l
  }
#+end_src
#+begin_export latex
\subsubsection{Method \ty{newSeg}}
Constructor method \ty{newSeg()} returns a segment of specified start
and length.
#+end_export
#+begin_src go <<Methods>>=
  func newSeg(x, y int) seg {
	  return seg{s:x, l:y}
  }
#+end_src
#+begin_export latex
\subsection{Data structure \ty{subject}} \label{Data structure subject}
This data structure describes a subject sequence, that is, a
reference, to which query sequences are compared. A \ty{subject}
contains:
\begin{itemize}
  \itemsep0em
  \item an enhanced suffix array (ESA), as described in the package
    \ty{esa}; note that an \ty{esa.Esa} structure has the field \ty{T}
    holding the original sequence;
  \item the total length of the sequence;
  \item the length of the forward strand of the sequence;
  \item the minimum anchor length for the sequence;
  \item headers of the contigs used to build the ESA;
  \item coordinates of the contigs on the \ty{Esa.T};
\end{itemize}
#+end_export
#+begin_src go <<Data structures>>=
  type subject struct {
	  esa *esa.Esa
	  totalL int
	  strandL int
	  a int
	  contigHeaders []string
	  contigSegments []seg
  }
#+end_src
#+begin_export latex
We import \ty{esa}.
#+end_export
#+begin_src go <<Imports>>=
  "github.com/evolbioinf/esa"
#+end_src
#+begin_export latex
\subsection{Data structure \ty{query}} \label{Data structure query}
The data structure \ty{query} describes a query sequence that is being
compared to the subject. A \ty{query} contains:
\begin{itemize}
  \itemsep0em
  \item the sequence as a slice of bytes;
  \item the total length of the sequence;
  \item a suffix of the sequence.
\end{itemize}
#+end_export
#+begin_src go <<Data structures>>=
  type query struct {
	  seq []byte
	  l int
	  suffix []byte
  }
#+end_src
#+begin_export latex
\subsubsection{Method \ty{updSuffix}}
This method updates the \ty{suffix} field with a suffix starting at
the position \ty{x}.
#+end_export
#+begin_src go <<Methods>>=
  func (q *query) updSuffix(x int) {
	  q.suffix = q.seq[x:]
  }
#+end_src
#+begin_export latex
\subsection{Data structure \ty{match}}
This custom data structure describes an exact match. A \ty{match}
contains:
\begin{itemize}
  \itemsep0em
  \item its length;
  \item starting positions in subject and query sequences;
  \item ending positions in subject and query sequences;
\end{itemize}
#+end_export
#+begin_src go <<Data structures>>=
  type match struct {
	  l int
	  startS int
	  startQ int
	  endS int
	  endQ int
  }
#+end_src
#+begin_export latex
\subsection{\textbf{Data structure \ty{Parameters}}} \label{Data structure Parameters}
!Fields of this data structure contain parameters used to call
!\ty{Intersect()}. The parameters include:
!
!1) a reference;
!2) paths to query genomes;
!4) a threshold, the minimum fraction of intersecting genomes;
!5) p-value of the shustring length (needed for \ty{sus.Quantile});
!6) a switch to print \ty{N} at the positions of mismatches;
!7) a number of threads. 

\ty{Intersect} is defined in Section~\ref{Intersect}.
#+end_export
#+begin_src go <<Data structures>>=
  type Parameters struct{
	  Reference []*fasta.Sequence
	  QueryPaths []string
	  Threshold float64
	  ShustrPval float64
	  PrintN bool
	  NumThreads int
  }
#+end_src
#+begin_export latex
We import \ty{fasta}.
#+end_export
#+begin_src go <<Imports>>=
  "github.com/evolbioinf/fasta"
#+end_src
#+begin_export latex
\subsection{\textbf{Function \ty{Intersect}}} \label{Intersect}
!The function \ty{Intersect} accepts a struct of \ty{Parameters} and
!returns sequences of homologous regions, common to subject
!(reference) and query sequences.

The data structure \ty{Parameters} is defined in Section~\ref{Data
  structure Parameters}. We 'unpack' the reference field of the
\ty{Parameters}. Then we check and count query files. If there are
none, we don't have the material to intersect with, thus we return the
original sequence. If the files do exist, we prepare the subject from
the reference and find common homologous regions.
#+end_export
#+begin_src go <<Functions>>=
  func Intersect(parameters Parameters) []*fasta.Sequence {
	  r := parameters.Reference
	  numFiles := 0
	  //<<Check and count queries>>
	  if numFiles == 0 {
		  return r
	  } else {
		  //<<Prepare the \ty{subject}>>
		  //<<Find homologous regions>>
		  //<<Find common homologous regions>>
	  }
  }
#+end_src
#+begin_export latex
We iterate over the slice of query paths, checking if they exist with
the function \ty{fileExists}, which we will define shortly. If a query
file exists, we add one to the number of files. Otherwise we print an
error and exit.
#+end_export
#+begin_src go <<Check and count queries>>=
  for _, queryPath := range parameters.QueryPaths {
	  exists, err := fileExists(queryPath)
	  if err != nil {
		  fmt.Fprintf(os.Stderr,
		  "chr.Intersect: error checking query file %s: %v",
			  queryPath, err)
		  os.Exit(1)
	  }

	  if exists {
		numFiles++
	  } else {
		  fmt.Fprintf(os.Stderr,
			  "chr.Intersect: query file %s does not exist",
			  queryPath)
		  os.Exit(1)
	  }
  }
#+end_src
#+begin_export latex
We import \ty{os} and \ty{fmt}.
#+end_export
#+begin_src go <<Imports>>=
  "os"
  "fmt"
#+end_src
#+begin_export latex
The function \ty{fileExists} checks if the file exists.
#+end_export
#+begin_src go <<Functions>>=
  func fileExists(path string) (bool, error) {
      _, err := os.Stat(path)
      if err == nil {
	  return true, nil
      }
      if os.IsNotExist(err) {
	  return false, nil
      }
      return false, err
  }
#+end_src
#+begin_export latex
\subsubsection{Prepare the subject} \label{Prepare the subject}
The preparation of the subject comprises the following steps:
\begin{itemize}
  \itemsep0em
  \item initialize a \ty{subject} struct (Section~\ref{Data structure
    subject}) and some of its fields;
  \item normalize the subject's contigs;
  \item process the subject's contigs;
  \item calculate the minimum anchor length;
  \item build an ESA;
  \item populate the \ty{subject} struct.
\end{itemize}
#+end_export
#+begin_src go <<Prepare the \ty{subject}>>=
  var subject subject
  //<<Normalize the contigs>>
  //<<Process the contigs>>
  //<<Calculate the minimum anchor length>>
  //<<Build subject's ESA>>
  //<<Populate \ty{subject}>>
#+end_src
#+begin_export latex
We normalize the contigs, that is, remove non-canonical nucleotides
and convert nucleotides to uppercase.
#+end_export
#+begin_src go <<Normalize the contigs>>=
  for i, _ := range r {
	  fastautils.Clean(r[i])
	  fastautils.DataToUpper(r[i])
  }
#+end_src
#+begin_export latex
We import \ty{fastautils}.
#+end_export
#+begin_src go <<Imports>>=
  "github.com/ivantsers/fastautils"
#+end_src
#+begin_export latex
We extract the first subject sequence's \ty{Header} and
\ty{Data}. Thereafter, we initialize fields describing the subject's
contigs. If there is more than one contig, we concatenate all contigs
into one sequence.
#+end_export
#+begin_src go <<Process the contigs>>=
  subjectHeader := r[0].Header()
  subjectData := r[0].Data()
  contigHeaders := []string{subjectHeader}
  contigSegs := []seg{newSeg(0, len(subjectData))}
  cL := len(subjectData)
  if len(r) > 1 {
	  //<<Concatenate subject's contigs>>
  }
#+end_src
#+begin_export latex
We traverse the slice of subject sequences, extract their headers and
data, and initialize a segment describing the current contig. Then we
concatenate the current sequence and the total subject sequence we
have accumulated so far.
#+end_export
#+begin_src go <<Concatenate subject's contigs>>=
  for i := 1; i < len(r); i++ {
	  seq := r[i]
	  seqH := seq.Header()
	  seqD := seq.Data()
	  seqL := len(seqD)
	  var cseg seg
	  //<<Perform the concatenation>>
  }
#+end_src
#+begin_export latex
We append the header to the slice of contig headers. Then we append an
exclamation mark to the combined subject data and increment the total
length by one. Then we create a new segment for the next contig right
after the exclamation mark and save it. After that, we append the
contig's data to the subject and increment the total length.
#+end_export
#+begin_src go <<Perform the concatenation>>=
  contigHeaders = append(contigHeaders, seqH)
  subjectData = append(subjectData, '!')
  cL += 1
  cseg = newSeg(cL + 1, seqL - 1)
  contigSegs = append(contigSegs, cseg)
  subjectData = append(subjectData, seqD...)
  cL += seqL
#+end_src
#+begin_export latex
We calculate the GC content of the subject and use it to find the
length of a non-random shustring. The latter task is delegated to
\ty{sus.Quantile}, which we use with the shustring p-value specified
in \ty{parameters.ShustrPval} that we also refer to as \ty{pval}.
#+end_export
#+begin_src go <<Calculate the minimum anchor length>>=
  numBases := float64(len(subjectData))
  numGC := 0.0
  for _, c := range subjectData {
	  if c == 'C' || c == 'G' {
		  numGC++
	  }
  }
  gc := numGC/numBases
  pval := parameters.ShustrPval
  minAncLen := sus.Quantile(cL, gc, pval)
#+end_src
#+begin_export latex
We import \ty{sus}.
#+end_export
#+begin_src go <<Imports>>=
  "github.com/evolbioinf/sus"
#+end_src
#+begin_export latex
We calculate the reverse strand and append it to the subject. We
separate the strands with a sentinel character \ty{\#}. Then we use the
two-stranded subject to build an ESA.
#+end_export
#+begin_src go <<Build subject's ESA>>=
  rev := fasta.NewSequence("reverse", subjectData)
  rev.ReverseComplement()
  subjectData = append(subjectData, '#')
  subjectData = append(subjectData, rev.Data()...)
  sa := esa.MakeEsa(subjectData)
#+end_src
#+begin_export latex
We populate the \ty{subject} struct with the calculated values.
#+end_export
#+begin_src go <<Populate \ty{subject}>>=
  subject.esa = sa
  subject.totalL = len(subjectData)
  subject.strandL = subject.totalL / 2
  subject.a = minAncLen
  subject.contigHeaders = contigHeaders
  subject.contigSegments = contigSegs
#+end_src
#+begin_export latex
\subsubsection{Find homologous regions}
In this section, we search for homologous regions in multiple queries
and append the results to the same struct of \ty{homologs}
(Figure~\ref{fig:findHomologs}).

\begin{figure}[H]
    \includegraphics[width=0.9\linewidth]{figFindHomologs.eps}
    \caption{Data flow during the search for homologous regions in
      queries. The \ty{findHomologs} function identifies exact matches
      in a query, selects those that meet anchor criteria, and uses
      these anchors to build segments, which are then filtered for
      overlaps. This process is repeated for each query, and the
      filtered homologs are added to a common struct that holds all
      homologies.}
    \label{fig:findHomologs}
\end{figure}
We prepare for a concurrent processing of queries. We create a
workgroup, a \ty{Mutex}, and a file channel needed for a concurrent
processing of queries. Finally, we initialize an ultimate slice to
store all homologs.

We proceed with the definition the \ty{worker} function. Then we check
the number of threads and start the number of \ty{workers} limited by
the number of threads specified in the \ty{parameters} struct. The
workers will receive tasks (file names) from the channel. Once the
workers are ready, we send query file names to the channel. In the
end, we wait all workers to finish.
#+end_export
#+begin_src go <<Find homologous regions>>=
  // Prepare to process queries concurrently>>
    var wg sync.WaitGroup
    var mu sync.Mutex
    fileChan := make(chan string)
    homologs := Homologs{S: []seg{}, N: make(map[int]bool)}
  //<<Define the \ty{worker} function>>
  //<<Check \ty{numThreads}>>
  // Start workers
  for i := 0; i < numThreads; i++ {
	  wg.Add(1)
	  go worker()
  }
  //<<Send files to the file channel>>
  // Wait for all workers to finish
  wg.Wait()
#+end_src
#+begin_export latex
We import \ty{sync}.
#+end_export
#+begin_src go <<Imports>>=
  "sync"
#+end_src
#+begin_export latex
A worker takes a query file from the channel, prepares the query for
the analysis, calls the \ty{findHomologs} function (which is not
written yet), and safely appends its output to the struct of
\ty{homologs} using the \ty{Mutex}. Segregating site positions are
appended the the function \ty{appendKeys}, which we will define
shortly.
#+end_export
#+begin_src go <<Define the \ty{worker} function>>=
  worker := func() {
	  defer wg.Done()
	  for file := range fileChan {
		  //<<Prepare the \ty{query}>>
		  h := findHomologs(query, subject)
		  mu.Lock()
		  homologs.S = append(homologs.S, h.S...)
		  homologs.N = appendKeys(homologs.N, h.N)
		  mu.Unlock()
	  }
  }
#+end_src
#+begin_export latex
The function \ty{appendKeys} adds \ty{int} keys from the map \ty{b} to
the map \ty{a}. Both maps hold values of type \ty{bool}.
#+end_export
#+begin_src go <<Functions>>=
  func appendKeys(a map[int]bool, b map[int]bool) map[int]bool {
	  for key, _ := range(b) {
		  a[key] = true
	  }
	  return a
  }
#+end_src
#+begin_export latex
We make sure that the value of the \ty{numThreads} field of the
\ty{parameters} struct is always greater than zero.
#+end_export
#+begin_src go <<Check \ty{numThreads}>>=
  numThreads := parameters.NumThreads
  if numThreads <= 0 {
	  numThreads = 1
  }
#+end_src
#+begin_export latex
We iterate over the entries in the target directory, build paths to
the files and send them to the file channel.
#+end_export
#+begin_src go <<Send files to the file channel>>=
  go func() {
	  for _, queryPath := range parameters.QueryPaths {	
		  fileChan <- queryPath
	  }
	  close(fileChan)
  }()
#+end_src
#+begin_export latex
Let us return to the \ty{worker}'s tasks. We initialize and populate a
\ty{query} struct (Section~\ref{Data structure query}). We build a
path to a query file, read all sequences from it, concatenate and
normalize them (clean and convert to uppercase). Then we set the
values for the \ty{seq} and \ty{l} fields.
#+end_export
#+begin_src go <<Prepare the \ty{query}>>=
  var query query
  f, _ := os.Open(file)
  queryData := fastautils.ReadAll(f)
  f.Close()
  qSeq, err := fastautils.Concatenate(queryData, '?')
  //<<Respond to the error of sequence concatenation>>
  fastautils.Clean(qSeq)
  fastautils.DataToUpper(qSeq)
  query.seq = qSeq.Data()
  query.l = len(qSeq.Data())
#+end_src
#+begin_export latex
We print the error and exit.
#+end_export
#+begin_src go <<Respond to the error of sequence concatenation>>=
  if err != nil {
	  fmt.Fprint(os.Stderr, err)
	  os.Exit(1)
  }
#+end_src
#+begin_export latex
The function \ty{findHomologs} accepts structs of \ty{subject} and
\ty{query}. The function finds homologous regions based on exact
matches between the \ty{subject} and the \ty{query}. The function
returns a \ty{Homologs} struct.

We initialize variables to operate with during the search for homologs
and conduct the anchor search. Then we filter overlaps and update the
map of segregating sites \ty{h.N} with the segregating sites from the
filtered homologous segments. We do this with help of the function
\ty{appendKeys}. Finally, we return the results of the search. If no
homologs were found, we return an empty \ty{Homologs} slice, which is
still a valid result.
#+end_export
#+begin_src go <<Functions>>=
  func findHomologs(query query, subject subject) Homologs { 
	  h := Homologs{S: []seg{}, N: make(map[int]bool)}
	  //<<Initialize the search for homologs>>
	  //<<Anchor search>>
	  h.filterOverlaps()
	  for _, seg := range h.S {
		  h.N = appendKeys(h.N, seg.n)
	  }
	  return h
  }
#+end_src
#+begin_export latex
We declare variables to operate with during the search. These are:
\begin{itemize}
  \itemsep0em
  \item current and previous positions in the query;
  \item current and previous \ty{matches}:
  \item a \ty{segment};
  \item a map of segregating sites found within a suffix of the query;
  \item an indicator as to whether the right anchor exists;
\end{itemize}
#+end_export
#+begin_src go <<Initialize the search for homologs>>=
  var qc, qp int
  var c, p match
  seg := seg{s:0, l:0, n: make(map[int]bool)}
  rightAnchor := false
#+end_src
#+begin_export latex
We perform the anchor search in a suffix of our query. We update the
\ty{suffix} field of the \ty{query} with a suffix starting at the
current position and ends at the last byte of the query sequence. We
clear the map \ty{segN}, in which we keep segregating sites found in
the current suffix. This map serves as a "buffer" separating the
results of segregation site calling in the current suffix from
\ty{h.N} (Figure~\ref{fig:FHlogic}).

Then we search for an anchor. There are two scenarios: 1) there is a
match that starts right after the last mismatch, 2) there is a distant
match somewhere else in the ESA. There are two functions to be
written.

In the first case, we try to find the longest common prefix (LCP) at
the given positions in query and subject sequences using the function
\ty{lcpAnchor}. In the second case, we turn to the ESA and look for a
matching suffix using the function \ty{esaAnchor}. Both functions
advance in the subject and update the current match \ty{c}. Both
functions return \ty{true} if a significant match is found.

To optimize the search, we put both functions into a short-circuit
evaluation, that is, \ty{esaAnchor} is called only if \ty{lcpAnchor}
has returned \ty{false}~\cite{kloe20:alg}. If a match is found, we
proceed with calculating the end positions of the previous match in
the query and the subject. Then we analyze the current match and
decide whether the current segment can be extended with it. If so, we
extend the current segment. If it cannot be extended and the right
anchor is found, we open a new segment and save the current one in
\ty{h.S}. After these operations, we remember the current match \ty{c}
and jump in the query by at least one nucleotide, which is most likely
to be a mutation.
#+end_export
#+begin_src go <<Anchor search>>=
  for qc < query.l {
	  query.updSuffix(qc)
	  if <<LCP anchor>> || <<ESA anchor>> {
	  p.endQ = qp + p.l
	  p.endS = p.startS + p.l	
	  //<<Analyze current match>>
	  if segCanBeExtended {
		  //<<Extend current segment>>
	  } else {
		  if rightAnchor || p.l / 2 >= subject.a {
			  //<<Close current segment>>
		  }
		  //<<Open a new segment>>
	  }
	  //<<Remember current match>>
  }
	  qc = qc + c.l + 1
  }
  //Close the last segment if open:
  if rightAnchor || p.l / 2 >= subject.a {
	  //<<Close current segment>>
  }
#+end_src
#+begin_export latex
We call \ty{lcpAnchor}, which we will define shortly.
#+end_export
#+begin_src go <<LCP anchor>>=
  lcpAnchor(&c, &p, query, subject, qc, qp)
#+end_src
#+begin_export latex
The function \ty{lcpAnchor} accepts the following inputs:
\begin{enumerate}
  \itemsep0em
  \item a pointer to the current match \ty{c};
  \item a ponter to the previous match \ty{p};
  \item a \ty{query} struct;
  \item a \ty{subject} struct;
  \item current and previous positions in the query.
\end{enumerate}

Regardless of the significance of the match, the function updates the
start and the length of the current match.

First, the function calculates the gap between the current and
previous positions in the query and determines a position in the
subject to attempt matching the longest common prefix (LCP). If this
position is beyond the end of the subject or if the gap is larger than
the minimum anchor length (meaning it's not random), the function
returns \ty{false}. Otherwise, it updates the position in the subject
and finds the LCP length at this position. The length of the current
match is then updated with the found length. If this length meets the
minimum anchor length requirement, the match is considered
significant, and the function returns \ty{true}.
#+end_export
#+begin_src go <<Functions>>=
  func lcpAnchor(c *match, p *match,
	  query query, subject subject,
	  qc, qp int) bool {
	  advance := qc - qp
	  gap := advance - p.l
	  tryS := p.startS + advance
	  if tryS >= subject.totalL || gap > subject.a {                          
		  return false
	  }
	  c.startS = tryS
	  newL := lcpLen(query.l, query.suffix, subject.esa.T[tryS:])
	  c.l = newL
	  return newL >= subject.a
  }
#+end_src
#+begin_export latex
The function \ty{lcpLen} returns the length of the longest common
prefix of two slices of bytes. The auxillary function \ty{min} returns
the smallest of integers.
#+end_export
#+begin_src go <<Functions>>=
  func lcpLen(max int, a, b []byte) int {
      limit := min(len(a), len(b), max)
      count := 0
      for i := 0; i < limit; i++ {
	  if a[i] != b[i] {
	      break
	  }
	  count++
      }
      return count
  }

  func min(vals ...int) int {
      m := vals[0]
      for _, v := range vals {
	  if v < m {
	      m = v
	  }
      }
      return m
  }
#+end_src
#+begin_export latex
We call \ty{esaAnchor} to find a match in the ESA.
#+end_export
#+begin_src go <<ESA anchor>>=
  esaAnchor(&c, query, subject)
#+end_src
#+begin_export latex
The function \ty{esaAnchor} accepts:

\begin{enumerate}
  \itemsep0em
  \item a pointer to the current match \ty{c};
  \item the \ty{query} struct;
  \item the \ty{subject} struct.
\end{enumerate}

The function returns a boolean. Regardless of the significance of the
match, the function updates the start and the length of the current
match.

A match is considered significant if it is unique and not shorter than
the minimum anchor length. A match is unique if it starts and ends at
the same position in the subject's ESA. In terms of a suffix tree, a
unique match ends on a leaf.
#+end_export
#+begin_src go <<Functions>>=
  func esaAnchor(c *match, query query, subject subject) bool {
	  mc := subject.esa.MatchPref(query.suffix)
	  newStartS := subject.esa.Sa[mc.I]
	  newL := mc.L
	  c.startS = newStartS
	  c.l = newL
	  lu := (mc.J == mc.I) && (newL >= subject.a)
	  return lu
  }
#+end_src
#+end_export
#+begin_export latex
We determine if the match:
\begin{itemize}
  \itemsep0em
  \item starts in the subject after the previous match;
  \item is equidistant with the previous match in the subject and
    query;
  \item is located on the same strand in the subject as the previous
    match.
\end{itemize}
If these criteria are met, we label the current segment as
extendible.
#+end_export
#+begin_src go <<Analyze current match>>=
  afterPrev := c.startS > p.endS

  areEquidist := qc - p.endQ == c.startS - p.endS

  onSameStrand := (c.startS < subject.strandL) ==
		  (p.startS < subject.strandL)

  segCanBeExtended := afterPrev &&
		      areEquidist &&
		      onSameStrand
#+end_src
#+begin_export latex
We extend the segment to the end of the new anchor. We calculate the
length of the inter-anchor gap. Then we extend the current segment
with this gap length and the length of the current match. After the
extension, we add new positions to the map of mismatches \ty{segN}
as keys. We also remember that we have just found a new right anchor.
#+end_export
#+begin_src go <<Extend current segment>>=
  prevSegEnd := seg.end()
  gapLen := qc - p.endQ
  seg.l = seg.l + gapLen + c.l
  //<<Add positions to \ty{seg.n}>>
  rightAnchor = true
#+end_src
#+begin_export latex
We extract sequences located between the current pair of anchors from
the subject and the query. Then we compare each nucleotide using the
function \ty{compare}, which we will define shortly. We save the
mismatches' positions to the map \ty{seg.n}. If mismatches are found
on the reverse strand, we project their coordinates onto the forward
strand.
#+end_export
#+begin_src go <<Add positions to \ty{seg.n}>>=
  gapSeqSubject := subject.esa.T[prevSegEnd:prevSegEnd + gapLen]
  gapSeqQuery := query.seq[p.endQ:p.endQ + gapLen]
  for i := 0; i < gapLen; i++ {
	  isSegsite := compare(gapSeqSubject[i], gapSeqQuery[i])
	  ssPos := -1
	  if isSegsite {
		  if prevSegEnd > subject.strandL {
			  ssPos = subject.totalL - prevSegEnd - i - 1 
			  seg.n[ssPos] = true
		  } else {
			  ssPos = prevSegEnd + i
			  seg.n[ssPos] = true
		  }
	  }
  }
#+end_src
#+begin_export latex
The function \ty{compare} compares two bytes and returns \ty{true} if
they are not equal.
#+end_export
#+begin_src go <<Functions>>=
  func compare(s byte, q byte) bool {
	  notEqual := true
	  if s == q {
		  notEqual = false
	  }
	  return notEqual
  }
#+end_src
#+begin_export latex
To close the current segment is to project it onto the forward strand
(if necessary) and append it to the struct of homologies.
#+end_export
#+begin_src go <<Close current segment>>=
  if seg.s > subject.strandL {
	  seg.s = subject.totalL - seg.s - seg.l
  }
  h.S = append(h.S, seg)
#+end_src
#+begin_export latex
To open a segment is to declare its start and length, reset the
segregating site map, and forget that the right anchor was found.
#+end_export
#+begin_src go <<Open a new segment>>=
  seg.s = c.startS
  seg.l = c.l
  seg.n = make(map[int]bool)
  rightAnchor = false
#+end_src
#+begin_export latex
We update the previous position in the query and the previous match.
#+end_export
#+begin_src go <<Remember current match>>=
  qp = qc
  p.l = c.l
  p.startS = c.startS
#+end_src
#+begin_export latex
We start the worker functions. Their number is limited with
\ty{numThreads}.
#+end_export
#+begin_src go <<Start workers>>=

#+end_src
#+begin_export latex
\subsubsection{Find common homologous regions}
We use the set of homologous regions that we have discovered so far to
identify regions of the subject that are found in a given fraction of
the target genomes. We interpret the threshold fraction (a
\ty{Parameters} field, Section~\ref{Data structure Parameters}), then
we calculate pile heights, that is, how many times a nucleotide of the
subject is covered by piled-up homologous regions. Thereafter, we
convert them to segments, and then get the actual sequences
corresponding to the segments. This task is delegated to the function
\ty{pileHeights}, which is yet to be written.
#+end_export
#+begin_src go <<Find common homologous regions>>=
  //<<Interpret the threshold fraction>>
  p := pileHeights(homologs, subject.strandL)
  //<<Convert pile heights to homologous segments>>
  //<<Convert homologs to sequences>>
#+end_src
#+begin_export latex
We calculate the product of the threshold fraction $f$
(\ty{parameters.Threshold}) and $g$, which is the total number of
genomes being analyzed \textit{excluding} the implied subject. This
number corresponds to the number of files in the \ty{TargetDir} given
that the directory does not contain the reference.

We round the product down to the nearest integer, but if the latter
happens to be zero, we set the value to 1. Thus, we will be always
trying to find the complete or partial intersection for any $0 < f \le
1$ and $g \ge 1$.
#+end_export
#+begin_src go <<Interpret the threshold fraction>>=
  f := parameters.Threshold
  g := numFiles
  t := int(math.Floor(f * float64(g)))
  if t == 0 {
	  t = 1
  }
#+end_src
#+begin_export latex
We import \ty{math}.
#+end_export
#+begin_src go <<Imports>>=
  "math"
#+end_src
#+begin_export latex
\subsubsection{Calculate pile heights}
The function \ty{pileHeights} counts how many times a position in the
subject is covered by homologous segments found in queries.
#+end_export
#+begin_src go <<Functions>>=
  func pileHeights(h Homologs, strandL int) []int {
	  pile := make([]int, strandL)
	  for i := 0; i < len(h.S); i++ {
		  seg := h.S[i]
		  for j := seg.s; j < seg.end(); j++ {
			  pile[j] += 1
		  }
	  }
	  return pile
  }
#+end_src
#+begin_export latex
\subsubsection{Convert the pile heigths to homologous segments}
Once we have pile heights at hand, we can convert them to actual
segments of the reference. To avoid chimeric sequences in the output,
we create two maps: \ty{isAdj} and \ty{contigBounds}. The first
contains positions of adjacent homologous segments, and the second
contains positions of contig separators (exlcamation marks).  Once the
maps are created, we convert the pile height to subject segments and
return the intersection.
#+end_export
#+begin_src go <<Convert pile heights to homologous segments>>=
  //<<Create map of adjacent segments>>
  //<<Create map of contig bounds>>
  intersection := pileToSeg(p, t, isAdj, contigBounds)
  homologs.S = intersection
#+end_src
#+begin_export latex
When reference has been cleaned, two adjacent homologous segments can
be merged into one. The cleaning implies removal of non-canonical
nucleotides, that is, keeping only \ty{A, T, G, C}. The removal shifts
proper nucleotides that followed the removed characters to the
left. As the result, two regions separated in the original sequence
become adjacent in the cleaned sequence (Figure~\ref{fig:MatchEnds},
A). This creates a risk of an incorrect interpretation of pile heights
unless match ends are not neglected (Figure~\ref{fig:MatchEnds}, B).

\begin{figure}[H]
    \includegraphics[width=\linewidth]{figMatchEnds.eps}
    \caption{Normalization of the subject affects how pile heights are
      interpreted. In panel A, a region of the real SARS-CoV-2 genome
      from \ty{data/i/sars/t1.fasta} is used as a subject (reference)
      to find matches in 4 queries. This region contains 20
      undetermined nucleotides (\ty{N}), which are removed during
      normalization. The right fragment then gets a new start
      coordinate (shown in red). In panel B, the normalized sequence
      shows pile heights of 4 (grey), indicating that these
      nucleotides are present at the same positions in all 4
      queries. The red line marks the right border of the segment
      ending with \ty{TTTGT}. If the pile heights were converted to
      the final output without considering this border, they would
      produce a chimeric sequence, which does not exist in the
      reference.}
    \label{fig:MatchEnds}
\end{figure}

We create a map of adjacent segments. This map holds ends of the left
segments from pairs. The creation of this map is delegated to the
\ty{makeMapAdj} function, which is not written so far.
#+end_export
#+begin_src go <<Create map of adjacent segments>>=
  isAdj := makeMapAdj(homologs)
#+end_src
#+begin_export latex
We define the function \ty{makeMapAdj}. We search for such pairs of
segments, where the end of the left segment is immideately followed by
the start of the right segment. In terms of end-exclusive coordinates,
this implies that the start and the end share the same coordinate.

We declare two auxullary variables, \ty{starts} (a map of booleans
with integer keys) and \ty{ends} (a slice of integers). Then we use
them to find ends that coincide with at least one start. We save such
ends in the adjacency map.
#+end_export
#+begin_src go <<Functions>>=
  func makeMapAdj(h Homologs) map[int]bool {
	  //<<Describe starts and ends>>
	  isAdj := make(map[int]bool)
	  for _, e := range ends {
		  if starts[e] {
			  isAdj[e] = true
		  }
	  }
	  return isAdj
  }
#+end_src
#+begin_export latex
We traverse the slice of segments, saving each segment's start to the
map \ty{starts} and each end to the slice of \ty{ends}.
#+end_export
#+begin_src go <<Describe starts and ends>>=
  starts := make(map[int]bool)
  ends := []int{}
  for i := 0; i < len(h.S); i++ {
	  seg := h.S[i]
	  starts[seg.s] = true
	  ends = append(ends, seg.end())
  }
#+end_src
#+begin_export latex
If the subject is highly fragmented and query genomes are continious,
homologous segments may span over several subject sequences. In this,
case, the intersection will contain chimeric anonymous sequences
formally not present in the original input. To avoid such a scenario,
we create a map of subject contig bounds. Keys of this map correspond
to position of contig separators in the concatenated subject.
#+end_export
#+begin_src go <<Create map of contig bounds>>=
  contigBounds := make(map[int]bool)
  for _, contig := range subject.contigSegments {
	  contigBounds[contig.end() + 1] = true
  }
#+end_src
#+begin_export latex
The function \ty{pileToSeg} returns a slice of segments, where each
position is covered at least a given number of times. We iterate over
keys and values of the pile, skipping keys listed in the map of contig
bounds. We close an opened segment if the pile height is below the
threshold or the next position is outside of a contig. Otherwise we
extend the segment. If there is no opened segment, we open it. In the
end we close the last segment and return the result.
#+end_export
#+begin_src go <<Functions>>=
  func pileToSeg(p []int, t int,
	  isAdj map[int]bool,
	  contigBounds map[int]bool) []seg {
	  var segs []seg
	  var seg seg
	  segIsOpen := false
	  for k, v := range p {
		  if contigBounds[k] {
			  continue
		  }
		  if segIsOpen {
			  if v < t || contigBounds[k + 1] {
				  //<<Close intersection segment>>
			  } else {
				  //<<Extend intersection segment>>
			  }
		  } else {
			  //<<New intersection segment>>
		  }
	  }
	  if segIsOpen {
		  segs = append(segs, seg)
	  }
	  return segs
  }
#+end_src
#+begin_export latex
To close a segment is to append it to the resulting slice and toggle
segment's status.
#+end_export
#+begin_src go <<Close intersection segment>>=
  segs = append(segs, seg)
  segIsOpen = false
#+end_src
#+begin_export latex
The extension implies adding one to the segment's length. After the
extension we check whether the next position is listed in the segment
adjacency map. If so, we close the segment from here.
#+end_export
#+begin_src go <<Extend intersection segment>>=
  seg.l += 1
  if isAdj[k+1] {
	  //<<Close intersection segment>>
  }
#+end_src
#+begin_export latex
We open a new segment if the pile height is not less than the
threshold.
#+end_export
#+begin_src go <<New intersection segment>>=
  if v >= t {
	  seg.s = k
	  seg.l = 1
	  segIsOpen = true
  }
#+end_src
#+begin_export latex
\subsubsection{Convert the homologs to sequences}
We retrieve the formatting switches from the \ty{parameters} struct
and call the function \ty{homologsToFasta}, which is yet to be
written. The output of this function is the \ty{return} of
\ty{Intersect} (Section~\ref{Intersect})!
#+end_export
#+begin_src go <<Convert homologs to sequences>>=
  printN := parameters.PrintN
  result := homologsToFasta(homologs, subject, printN)
  return result
#+end_src
#+begin_export latex
The function \ty{homologsToFasta()} accepts a struct of \ty{Homologs},
a struct of \ty{subject}, and \ty{bool} switches to format the output
sequence and headers. The function returns a slice of pointers to
fasta entries (type \ty{fasta.Sequence}).

We initialize the output slice, then convert each segment of the input
\ty{Homologs} into a \ty{fasta.Sequence}. For this, we construct its
data, its header, and use them to make a sequence.
#+end_export
#+begin_src go <<Functions>>=
  func homologsToFasta(h Homologs, subject subject,
	  printN bool) []*fasta.Sequence {

	  var sequences []*fasta.Sequence
	  segs := h.S
	  ns := h.N
	  for num_seg, seg := range segs {
		  //<<Construct sequence data>>
		  //<<Construct sequence header>>
		  seq := fasta.NewSequence(header, data)
		  sequences = append(sequences, seq)		
	  }
	  return sequences
  }
#+end_src
#+begin_export latex
As the majority of bytes are not \ty{N}s, we copy the part of the
subject's sequence directly, and only then check if we need to print
\ty{N}s.

We get start and end positions of the current segment, initialize
a slice of bytes, and append bytes to it.
#+end_export
#+begin_src go <<Construct sequence data>>=
  start := seg.s
  end := seg.end()
  data := make([]byte, seg.l)
  copy(data, subject.esa.T[start:end])
  if printN {
	  //<<Replace segregating sites with \ty{N}s>>
  }
#+end_src
#+begin_export latex
We check if each position of the segment is listed in the map of
segregating sites. If so, we replace the byte with an \ty{N}.
#+end_export
#+begin_src go <<Replace segregating sites with \ty{N}s>>=
  for j := 0; j < seg.l; j++ {
	  if ns[start+j] {
		  data[j] = 'N'
	  }
  }
#+end_src
#+begin_export latex
We construct the sequence's header that contains the orginal contig's
header \ty{ch}, an underscore, and the segment's number \ty{sn}, for
example:
\begin{verbatim}
contig1_1
\end{verbatim}
To find \ty{sn}, we add one to the current segment index.  To find
\ty{sn}, we call \ty{findSegment}, which we still have to implement.
#+end_export
#+begin_src go <<Construct sequence header>>=
  sn := num_seg + 1
  ch := findSegment(seg, subject)
  header := fmt.Sprintf("%s_%d", ch, sn)
#+end_src
#+begin_export latex
\subsubsection{Auxillary functions for output formatting}
\textbf{\ty{findSegment}} accepts a segment, a struct of \ty{subject},
and a switch for processing shift fields. It finds the subject
contig, to which the segment belongs, and returns its header without
the shift fields and the coordinates of the segment on the contig.

We initialize the variables:
\begin{itemize}
  \itemsep0em
\item \ty{ch} for contig header,
\end{itemize}

After this, we 'unpack' some fields of the \ty{subject}. Then we check
whether our segment is located within some of the contigs using the
function \ty{startsWithin}, which we still need to write. If so, we
get the contig's header.

A segment can be found only once because the homologs are unique and
do not overlap, so we break as soon as we've found the contig it
belongs to.
#+end_export
#+begin_src go <<Functions>>=
  func findSegment(seg seg, subject subject) (string) {

	  var ch string
	  
	  contigHeaders := subject.contigHeaders
	  contigSegments := subject.contigSegments

	  for i, contigSeg := range contigSegments {
		  if startsWithin(seg, contigSeg) {
			  ch = contigHeaders[i]
			  break
		  }
	  }
	  return ch
  }
#+end_src
#+begin_export latex
\textbf{\ty{startsWithin}} checks if an inner \ty{seg} starts within
an outer \ty{seg}.
#+end_export
#+begin_src go <<Functions>>=
  func startsWithin(in seg, out seg) bool {
	  return in.s >= out.s
  }
#+end_src
