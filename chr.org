#+begin_export latex
\section{Implementation}
The package \ty{chr} has hooks for imports, data structures, methods,
and functions.
#+end_export
#+begin_src go <<chr.go>>=
  package chr
  import (
	  //<<Imports>>
  )
  //<<Data structures>>
  //<<Methods>>
  //<<Functions>>
#+end_src
#+begin_export latex
\subsection{\textbf{Data structure \ty{Homologs}}}
!Data structure \ty{Homologs} describes homologous regions of a
!subject found in queries. This data type contains a slice of segments
!of the subject \ty{S} and a map of segregating sites \ty{N}.

The data structure \ty{seg} contains the start \ty{s} and the length
\ty{l} of a segment, as defined in Section~\ref{Data structure seg}.
#+end_export
#+begin_src go <<Data structures>>=
  type Homologs struct {
	  S []seg
	  N map[int]bool
  }
#+end_src
#+begin_export latex
\subsubsection{Method \ty{sort}}
The method \ty{sort} orders \ty{Homologs} by their start positions.
#+end_export
#+begin_src go <<Functions>>=
  func (h *Homologs) sort() *Homologs {
	  sort.Slice(h.S, func(i, j int) bool {
		  return h.S[i].s < h.S[j].s
	  })
	  return h
  }
#+end_src
#+begin_export latex
We import \ty{sort}.
#+end_export
#+begin_src go <<Imports>>=
  "sort"
#+end_src
#+begin_export latex
\subsubsection{Method \ty{filterOverlaps}}
The method \ty{filterOverlaps} removes overlapping homologous regions
to create the longest possible chain of non-overlapping regions.

If slice \ty{Homologs.S} does not contain at least two elements, we
return early. If it does, we sort it and reduce overlapping stacks of
segments to the longest chain of co-linear non-overlapping segments
using the algorithm for two-dimensional fragment
chaining~\cite{ohl13:alg} as implemented in
\ty{phylonium}~\cite{kloe20:alg}. We initialize variables, then we
calculate chain scores, find links of the longest chain through
backtracking, and return the chain.
#+end_export
#+begin_src go <<Methods>>=
  func (h *Homologs) filterOverlaps() {
	  slen := len(h.S)
	  if slen < 2 {
		  return
	  }
	  h.sort()
	  segs := h.S
	  //<<Initialize chaining>>
	  //<<Calculate chain score>>
	  //<<Backtrack the chain>>
	  //<<Return the chain>>
  }
#+end_src
#+begin_export latex
We declare variables describing the chain. For each element of
\ty{segs}, we initialize:
\begin{itemize}
  \itemsep0em
  \item \ty{predecessor}---the previous segment in the longest chain
    ending before \ty{segs[i]});
  \item \ty{score}---the length of the longest chain ending at
    \ty{segs[i]}. The initial score of the chain is the first
    segment's length;
  \item \ty{visited}---whether \ty{segs[i]} has been visited during
    backtracking of chain links.
\end{itemize}
We also initialize the first elements for \ty{score} and
\ty{predecessor}.
#+end_export
#+begin_src go <<Initialize chaining>>=
  predecessor := make([]int, slen)
  score := make([]int, slen)
  visited := make([]bool, slen)
  score[0] = segs[0].l
  predecessor[0] = -1
#+end_src
#+begin_export latex
We calculate the chain score to maximize the number of non-overlapping
segments in \ty{segs}. Starting from the second segment, we traverse
each segment. For each \ty{segs[i]}, we find its preceding segment
\ty{segs[k]} that can form the longest chain ending before
\ty{segs[i]} starts.
#+end_export
#+begin_src go <<Calculate chain score>>=
  for i := 1; i < slen; i++ {
	  maxScore := 0
	  maxIndex := -1
	  for k := 0; k < i; k++ {
		  if segs[k].end() <= segs[i].s {
			  if score[k] > maxScore {
				  maxScore = score[k]
				  maxIndex = k
			  }
		  }
	  }
	  predecessor[i] = maxIndex
	  if maxIndex != -1 {
	      score[i] = segs[i].l + score[maxIndex]
	  } else {
	      score[i] = segs[i].l
	  }
  }
#+end_src
#+begin_export latex
We are to find \ty{s}, which is the index of the highest-scoring
segment in \texttt{segs}. This segment is the final link in the chain
we're looking for, and its score is the total score of the chain. To
find \ty{s}, we will use the \ty{argmax} function, which we will
define shortly. Once we identify \ty{s}, we go through the chain of
\ty{predecessor} links and mark the visited segments in \ty{visited}.
#+end_export
#+begin_src go <<Backtrack the chain>>=
  s := argmax(score)
  for s != -1 {
	  visited[s] = true
	  s = predecessor[s]
  }
#+end_src
#+begin_export latex
The function \ty{argmax} returns the index of the maximum value in the
input slice of integers.
#+end_export
#+begin_src go <<Functions>>=
  func argmax(x []int) int {
      maxIdx := 0
      for i := 1; i < len(x); i++ {
	  if x[i] > x[maxIdx] {
	      maxIdx = i
	  }
      }
      return maxIdx
  }
#+end_src
#+begin_export latex
We extract only visited elements from \ty{segs} and set the new value
of \ty{h.S}. We also reset the segregating site map if the related
segment was not visited to avoid reporting phantom mutations. We
finish this block with updating the fields of the homolog \ty{h}.
#+end_export
#+begin_src go <<Return the chain>>=
  var segred []seg
  for i := 0; i < slen; i++ {
	  if visited[i] {
		  segred = append(segred, segs[i])
	  }
  }
  h.S = segred
#+end_src
#+begin_export latex
\subsection{Data structure \ty{seg}} \label{Data structure seg}
We declare a custom data struct \ty{seg} to store segments of the
forward strand of the subject. A \ty{seg} contains a zero-based start
of a region of a subject sequence, its length, and a map \ty{n} to
store coordinates of segregating sites (on the subject) related to
this segment.
#+end_export
#+begin_src go <<Data structures>>=
  type seg struct {
	  s int
	  l int
	  n map[int]bool
  }
#+end_src
#+begin_export latex
\subsubsection{Method \ty{end}}
The method \ty{end()} returns an exclusive coordinate of the end of a
\ty{seg}. Thus, the package operates with zero-based, end-exclusive
coordinates.
#+end_export
#+begin_src go <<Methods>>=
  func (seg *seg) end() int {
	  return seg.s + seg.l
  }
#+end_src
#+begin_export latex
\subsubsection{Method \ty{newSeg}}
Constructor method \ty{newSeg()} returns a segment of specified start
and length.
#+end_export
#+begin_src go <<Methods>>=
  func newSeg(x, y int) seg {
	  return seg{s:x, l:y}
  }
#+end_src
#+begin_export latex
\subsection{Data structure \ty{subject}} \label{Data structure subject}
This data structure describes a subject sequence, that is, a
reference, to which query sequences are compared. A \ty{subject}
contains:
\begin{itemize}
  \itemsep0em
  \item an enhanced suffix array (ESA), as described in the package
    \ty{esa}; note that an \ty{esa.Esa} structure has the field \ty{T}
    holding the original sequence;
  \item the total length of the sequence;
  \item the length of the forward strand of the sequence;
  \item the minimum anchor length for the sequence;
  \item headers of the contigs used to build the ESA;
  \item coordinates of the contigs on the \ty{Esa.T};
  \item size of shifts applied to the contig starts (see Section~\ref{Shifts})
\end{itemize}
#+end_export
#+begin_src go <<Data structures>>=
  type subject struct {
	  esa *esa.Esa
	  totalL int
	  strandL int
	  a int
	  contigHeaders []string
	  contigSegments []seg
	  contigShifts map[string]int
  }
#+end_src
#+begin_export latex
We import \ty{esa}.
#+end_export
#+begin_src go <<Imports>>=
  "github.com/evolbioinf/esa"
#+end_src
#+begin_export latex
\subsection{Data structure \ty{query}} \label{Data structure query}
The data structure \ty{query} describes a query sequence that is being
compared to the subject. A \ty{query} contains:
\begin{itemize}
  \itemsep0em
  \item the sequence as a slice of bytes;
  \item the total length of the sequence;
  \item a suffix of the sequence.
\end{itemize}
#+end_export
#+begin_src go <<Data structures>>=
  type query struct {
	  seq []byte
	  l int
	  suffix []byte
  }
#+end_src
#+begin_export latex
\subsubsection{Method \ty{updSuffix}}
This method updates the \ty{suffix} field with a suffix starting at
the position \ty{x}.
#+end_export
#+begin_src go <<Methods>>=
  func (q *query) updSuffix(x int) {
	  q.suffix = q.seq[x:]
  }
#+end_src
#+begin_export latex
\subsection{Data structure \ty{match}}
This custom data structure describes an exact match. A \ty{match}
contains:
\begin{itemize}
  \itemsep0em
  \item its length;
  \item starting positions in subject and query sequences;
  \item ending positions in subject and query sequences;
\end{itemize}
#+end_export
#+begin_src go <<Data structures>>=
  type match struct {
	  l int
	  startS int
	  startQ int
	  endS int
	  endQ int
  }
#+end_src
#+begin_export latex
\subsection{\textbf{Data structure \ty{Parameters}}} \label{Data structure Parameters}
!Fields of this data structure contain parameters used to call
!\ty{Intersect()}. The parameters include:
!
!1) a reference;
!2) a switch to shift reference contig start coordinates to the rigth;
!3) path to the directory of target genomes \textit{minus the reference};
!4) threshold, the minimum fraction of intersecting genomes;
!5) p-value of the shustring length (needed for \ty{sus.Quantile});
!6) a switch to clean* subject sequence;
!7) a switch to clean* query sequences;
!8) a switch to print positions of segregating sites in output headers;
!9) a switch to print \ty{N} at the positions of mismatches;
!10) a switch to print one-based coordinates.
!*To clean a sequence is to remove non-ATGC nucleotides.

\ty{Intersect} is defined in Section~\ref{Intersect}.
#+end_export
#+begin_src go <<Data structures>>=
  type Parameters struct{
	  Reference []*fasta.Sequence
	  ShiftRefRight bool
	  TargetDir string
	  Threshold float64
	  ShustrPval float64
	  CleanSubject bool
	  CleanQuery bool
	  PrintSegSitePos bool
	  PrintN bool
	  PrintOneBased bool
  }
#+end_src
#+begin_export latex
We import \ty{fasta}.
#+end_export
#+begin_src go <<Imports>>=
  "github.com/evolbioinf/fasta"
#+end_src
#+begin_export latex
\subsubsection{Coordinate systems: parameter \ty{ShiftRefRight}}\label{Shifts}
The parameter \ty{ShiftRefRight} is useful when the \ty{Reference}
for \ty{Intersect} is a subset of the original sequence. Such a subset
can result from a sequence filtering, during which parts of the
original reference contigs are getting removed, causing a left shift
of the start coordinates in the downstream regions. An example of this
is the program \ty{fur}~\cite{hau21:fur}, which processes an original
sequence (a representative genome) by removing non-unique regions in
the Subtraction 1 step, often causing splits in the reference contigs.

If a subset of the original reference is used as the input for
\ty{Intersect}, the output coordinates cannot be directly compared to
the coordinates of the original contigs. In this case, we might want
the output to have coordinates compatible with the original
sequence. To make the coordinates compatible, we shift the output
coordinates to the right (Figure~\ref{fig:RightShift}).

\begin{figure}
  \centering
  \includegraphics[width=0.65\linewidth]{figRightShift.eps}
  \caption{Coordinates of output sequences $I$ yielded by an
    \ty{Intersect} run, where a subset $S$ of the original sequence
    $R$ was used as the \ty{Reference}. To make the coordinates of $I$
    compatible with the original contig $R$, we must adjust their
    positions on the contigs $s_1$ and $s_2$ by adding starts of the
    respective contigs to the starts of $I$ parts within these
    contigs. $R\{start,end\}$ show coordinates of $I$ on the original
    reference $R$; $s_n\{start, end\}$ show coordinates of $I$ on the
    subset $S$. \textbf{Note that \ty{Intersect} uses $S$ coordinates
      for all internal calculations, and the conversion to $R$
      coordinates happens only when the results are being printed.}}
  \label{fig:RightShift}
\end{figure}

By default (when \ty{ShiftRefRight} is \ty{false}), each reference
contig is treated by \ty{Intersect} as its own coordinate system,
starting at 0, with the (exclusive) end coinciding with the length of
the contig:
\begin{verbatim}
0..length
\end{verbatim}
If the parameter \ty{ShiftRefRight} is \ty{true}, we shift the output
start coordinates to the right. We cannot determine the size of the
shift without a direct comparison of the \ty{Reference} to the
original sequence. This would be a resource-intensive task to carry
out within the \ty{chr} package. However, it’s usually possible to
calculate the new coordinates before calling \ty{Intersect}. Thus, we
expect the new start coordinates to be included in the headers of the
subsetted reference in this format:
\begin{verbatim}
>originalContigName$n$x
\end{verbatim}
where \ty{n} is a number of the piece of the original contig, \ty{x}
is the start of the contig in the original reference. The \ty{n} field
ensures the uniqueness of the header name. We use the dollar sign
\ty{\$} to safely delimit the name and the coordinate. Thus,
\textbf{if you plan to use \ty{Intersect} on a subsetted reference but
  want the output to show the original coordinates, consider adding
  the suffix \ty{\$n\$x} (\ty{x} is zero-based) to the headers of the
  pre-processed sequences.} These will be recognized by \ty{Intersect}
when \ty{ShiftRefRight} is enabled.
#+end_export
#+begin_export latex
\subsection{\textbf{Function \ty{Intersect}}} \label{Intersect}
!The function \ty{Intersect} accepts a struct of \ty{Parameters} and
!returns sequences of homologous regions, common to subject
!(reference) and query sequences.

The data structure \ty{Parameters} is defined in Section~\ref{Data
  structure Parameters}. We 'unpack' some fields of the
\ty{Parameters} to make short aliases and set default values for some
fields. Then we count files in the specified target directory. If
there are none, we don't have the material to intersect with, thus we
return the original sequence. If the files do exist, we prepare the
subject from the reference and find common homologous regions.
#+end_export
#+begin_src go <<Functions>>=
  func Intersect(parameters Parameters) []*fasta.Sequence {
	  r := parameters.Reference
	  d := parameters.TargetDir
	  //<<Set default parameters>>
	  numFiles := 0
	  //<<Count files in \ty{d/}>>
	  if numFiles == 0 {
		  return r
	  } else {
		  //<<Prepare the \ty{subject}>>
		  //<<Find homologous regions>>
		  //<<Find common homologous regions>>
	  }
  }
#+end_src
#+begin_export latex
If the \ty{ShustrPval} field has not been set by the user, we set it
to the default \ty{0.95}.
#+end_export
#+begin_src go <<Set default parameters>>=
  if parameters.ShustrPval == 0.0 {
	  parameters.ShustrPval = 0.95
  }
#+end_src
#+begin_export latex
We open the directory and check for an error. If there is one, we
report it and stop. If there is no error, we count the files in the
target directory.
#+end_export
#+begin_src go <<Count files in \ty{d/}>>=
  dirEntries, err := os.ReadDir(d)
  if err != nil {
	  fmt.Fprintf(os.Stderr,
		  "chr.Intersect: error reading %v: %v", d, err)
	  os.Exit(1)
  }
  numFiles = len(dirEntries)
#+end_src
#+begin_export latex
We import \ty{os} and \ty{fmt}.
#+end_export
#+begin_src go <<Imports>>=
  "os"
  "fmt"
#+end_src
#+begin_export latex
\subsubsection{Prepare the subject} \label{Prepare the subject}
The preparation of the subject comprises the following steps:
\begin{itemize}
  \itemsep0em
  \item initialize a \ty{subject} struct (Section~\ref{Data structure
    subject}) and some of its fields;
  \item normalize the subject's contigs;
  \item process the subject's contigs;
  \item calculate the minimum anchor length;
  \item build an ESA;
  \item populate the \ty{subject} struct.
\end{itemize}
#+end_export
#+begin_src go <<Prepare the \ty{subject}>>=
  var subject subject
  subject.contigShifts = make(map[string]int)
  //<<Normalize the contigs>>
  //<<Process the contigs>>
  //<<Calculate the minimum anchor length>>
  //<<Build subject's ESA>>
  //<<Populate \ty{subject}>>
#+end_src
#+begin_export latex
We normalize the contigs. The complete normalization implies 1)
converting nucleotides to uppercase, and 2) removing non-canonical
nucleotides. We do the latter only if \ty{parameters.CleanSubject} has
been set to \ty{true}.
#+end_export
#+begin_src go <<Normalize the contigs>>=
  for i, _ := range r {
	  if parameters.CleanSubject {
		  fastautils.Clean(r[i])
	  }
	  fastautils.DataToUpper(r[i])
  }
#+end_src
#+begin_export latex
We import \ty{fastautils}.
#+end_export
#+begin_src go <<Imports>>=
  "github.com/ivantsers/fastautils"
#+end_src
#+begin_export latex
We 'unpack' the \ty{ShiftRefRight} fieild of the parameters, then
extract the first subject sequence's \ty{Header} and
\ty{Data}. Thereafter, we respond to \ty{ShiftRefRight} by calling the
function \ty{extractShiftField}, which we will define shortly. The
function will try to read the shift field \ty{\$x} from the header. We
proceed with the initialization of the the fields describing the
subject's contigs. If there is more than one contig, we concatenate
all contigs into one sequence.
#+end_export
#+begin_src go <<Process the contigs>>=
  shiftRefRight := parameters.ShiftRefRight
  subjectHeader := r[0].Header()
  subjectData := r[0].Data()
  if shiftRefRight {
	  err := extractShiftField(subjectHeader, &subject)
	  if err != nil {
		  fmt.Fprint(os.Stderr, err)
		  os.Exit(1)
	  }
  }
  contigHeaders := []string{subjectHeader}
  contigSegs := []seg{newSeg(0, len(subjectData))}
  cL := len(subjectData)
  if len(r) > 1 {
	  //<<Concatenate subject's contigs>>
  }
#+end_src
#+begin_export latex
\subsubsection{Read the reference contig shift}
The function \ty{extractShiftField} accepts a header and a pointer to
the \ty{subject} struct. It updates the \ty{subject.contigShifts} map
if the input header contains shift fields \ty{\$n\$x}
(Section~\ref{Shifts}). The function's only return is an error.

We initialize an error, then check whether our input variables are
empty. Then we split the header into fields by \ty{\$}, and create
error messages if there are problems with the splitting. Once we have
read the shift fields, we update the shifts map of the \ty{subject}
struct.
#+end_export
#+begin_src go <<Functions>>=
  func extractShiftField(header string, subject *subject) error {
	  var err error
	  //<<Are the inputs empty?>>
	  headerFields := strings.Split(header, "$")

	  if len(headerFields) < 3 || <<Any field is empty>>  {
		  //<<Error of splitting>>
	  }

	  shift, errConv := strconv.Atoi(headerFields[2])
	  if errConv != nil {
		  //<<Error of type conversion>>
	  }

	  subject.contigShifts[header] = shift

	  return err
  }

#+end_src
#+begin_export latex
We import \ty{strings} and \ty{strconv}.
#+end_export
#+begin_src go <<Imports>>=
  "strings"
  "strconv"
#+end_src
#+begin_export latex
We check whether the header is an empty string or the subject is
\ty{nil}. If so, we create the corresponding error message and return
early.
#+end_export
#+begin_src go <<Are the inputs empty?>>=
  if header == "" {
	  err = fmt.Errorf("chr.Intersect: failed " +
		  "extractShiftField: header is empty\n")
	  return err
  }
  if subject == nil {
	  err = fmt.Errorf("chr.Intersect: failed " +
		  "extractShiftField: subject is nil\n")
	  return err
  }
#+end_src
#+begin_export latex
We check if any of the fields of the header is empty.
#+end_export
#+begin_src go <<Any field is empty>>=
  headerFields[1] == "" || headerFields[2] == ""
#+end_src
#+begin_export latex
We create an error message for a failed field splitting.
#+end_export
#+begin_src go <<Error of splitting>>=
  err = fmt.Errorf(
	  "chr.Intersect: error reading " +
	  "shift fields in the reference " +
	  "header %v\n", header)
#+end_src
#+begin_export latex
We create an error message for a failed string to integer conversion
and extend it with the error of the \ty{strconv.Atoi}.
#+end_export
#+begin_src go <<Error of type conversion>>=
  err = fmt.Errorf(
	  "chr.Intersect: error converting " +
	  "a shift field into an integer:" +
	  "\n\t%v\n", errConv)
#+end_src
#+begin_export latex
We traverse the slice of subject sequences, extract their headers and
data, and initialize a segment describing the current contig. Then we
check whether \ty{ShiftRefRight} is \ty{true}. If so, we extract the
shift value from a header. Then we concatenate the current sequence
and the total subject sequence we have accumulated so far.
#+end_export
#+begin_src go <<Concatenate subject's contigs>>=
  for i := 1; i < len(r); i++ {
	  seq := r[i]
	  seqH := seq.Header()
	  seqD := seq.Data()
	  seqL := len(seqD)
	  var cseg seg // initialize a segment
	  if shiftRefRight {
		  err := extractShiftField(seqH, &subject)
		  if err != nil {
			  fmt.Fprint(os.Stderr, err)
			  os.Exit(1)
		  }
	  }
	  //<<Perform the concatenation>>
  }
#+end_src
#+begin_export latex
We append the header to the slice of contig headers. Then we append an
exclamation mark to the combined subject data and increment the total
length by one. The new total length us the start of the contig on the
concatenated subject data. We create a \ty{seg} and append it to the
slice of contig segments. Finally, we append the contig's data to the
combined subject's data and increment the total length.
#+end_export
#+begin_src go <<Perform the concatenation>>=
  contigHeaders = append(contigHeaders, seqH)
  subjectData = append(subjectData, '!')
  cL += 1
  cseg = newSeg(cL, seqL)
  contigSegs = append(contigSegs, cseg)
  subjectData = append(subjectData, seqD...)
  cL += seqL
#+end_src
#+begin_export latex
We calculate the GC content of the subject and use it to find the
length of a non-random shustring. The latter task is delegated to
\ty{sus.Quantile}, which we use with the shustring p-value specified
in \ty{parameters.ShustrPval} that we also refer to as \ty{pval}.
#+end_export
#+begin_src go <<Calculate the minimum anchor length>>=
  atgc := 0.0
  gc := 0.0
  for _, c := range subjectData {
	  if c == 'A' || c == 'C' || c == 'G' || c == 'T' {
		  atgc++
		    if c == 'C' || c == 'G' {
			    gc++
		    }
	  }
  }
  gcContent := gc/atgc
  pval := parameters.ShustrPval
  minAncLen := sus.Quantile(cL, gcContent, pval)
#+end_src
#+begin_export latex
We import \ty{sus}.
#+end_export
#+begin_src go <<Imports>>=
  "github.com/evolbioinf/sus"
#+end_src
#+begin_export latex
We calculate the reverse strand and append it to the subject. We
separate the strands with a sentinel character \ty{\#}. Then we use the
two-stranded subject to build an ESA.
#+end_export
#+begin_src go <<Build subject's ESA>>=
  rev := fasta.NewSequence("reverse", subjectData)
  rev.ReverseComplement()
  subjectData = append(subjectData, '#')
  subjectData = append(subjectData, rev.Data()...)
  sa := esa.MakeEsa(subjectData)
#+end_src
#+begin_export latex
We populate the \ty{subject} struct with the calculated values.
#+end_export
#+begin_src go <<Populate \ty{subject}>>=
  subject.esa = sa
  subject.totalL = len(subjectData)
  subject.strandL = subject.totalL / 2
  subject.a = minAncLen
  subject.contigHeaders = contigHeaders
  subject.contigSegments = contigSegs
#+end_src
#+begin_export latex
\subsubsection{Find homologous regions}
We initialize a slice of segments to store found homologous
regions. We iterate over the files, prepare \ty{queries}, and call the
\ty{findHomologs} function, which we still have to write. We append
the output of this function to the \ty{homologs} slice we have just
initialized (Figure~\ref{fig:findHomologs}).

\begin{figure}[H]
    \includegraphics[width=0.9\linewidth]{figFindHomologs.eps}
    \caption{Data flow during the search for homologous regions in
      queries. The \ty{findHomologs} function identifies exact matches
      in a query, selects those that meet anchor criteria, and uses
      these anchors to build segments, which are then filtered for
      overlaps. This process is repeated for each query, and the
      filtered homologs are added to a common slice that holds all
      homologies.}
    \label{fig:findHomologs}
\end{figure}
#+end_export
#+begin_src go <<Find homologous regions>>=
  homologs := Homologs{S: []seg{}, N: make(map[int]bool)}
  for _, entry := range dirEntries {
	  //<<Prepare a \ty{query}>>
	  h := findHomologs(query, subject)
	  homologs.S = append(homologs.S, h.S...)
	  homologs.N = appendKeys(homologs.N, h.N)
  }
#+end_src
#+begin_export latex
We initialize and populate a \ty{query} struct (Section~\ref{Data
  structure query}). We build a path to a query file, read all
sequences from it, concatenate and normalize them (clean if asked to,
convert to uppercase always). Then we set the values for the \ty{seq}
and \ty{l} fields.
#+end_export
#+begin_src go <<Prepare a \ty{query}>>=
  var query query
  filePath := d + "/" + entry.Name()
  f, _ := os.Open(filePath)
  queryData := fastautils.ReadAll(f)
  f.Close()
  qSeq, err := fastautils.Concatenate(queryData, '?')
  //<<Respond to the error of sequence concatenation>>
  if parameters.CleanQuery {
	  fastautils.Clean(qSeq)
  }
  fastautils.DataToUpper(qSeq)
  query.seq = qSeq.Data()
  query.l = len(qSeq.Data())
#+end_src
#+begin_export latex
We print the error and exit.
#+end_export
#+begin_src go <<Respond to the error of sequence concatenation>>=
  if err != nil {
	  fmt.Fprint(os.Stderr, err)
	  os.Exit(1)
  }
#+end_src
#+begin_export latex
The function \ty{findHomologs} accepts structs of \ty{subject} and
\ty{query}. The function finds homologous regions based on exact
matches between the \ty{subject} and the \ty{query}. The function
returns a \ty{Homologs} struct.

We initialize variables to operate with during the search for homologs
and conduct the anchor search. Then we filter overlaps and update the
map of segregating sites \ty{h.N} with the segregating sites from the
filtered homologous segments. We do this with help of the function
\ty{appendKeys}, which we will define later. Finally, we return the
results of the search. If no homologs were found, we return an empty
\ty{Homologs} slice, which is still a valid result.
#+end_export
#+begin_src go <<Functions>>=
  func findHomologs(query query, subject subject) Homologs { 
	  h := Homologs{S: []seg{}, N: make(map[int]bool)}
	  //<<Initialize the search for homologs>>
	  //<<Anchor search>>
	  h.filterOverlaps()
	  for _, seg := range h.S {
		  h.N = appendKeys(h.N, seg.n)
	  }
	  return h
  }
#+end_src
#+begin_export latex
We declare variables to operate with during the search. These are:
\begin{itemize}
  \itemsep0em
  \item current and previous positions in the query;
  \item current and previous \ty{matches}:
  \item a \ty{segment};
  \item a map of segregating sites found within a suffix of the query;
  \item an indicator as to whether the right anchor exists;
\end{itemize}
#+end_export
#+begin_src go <<Initialize the search for homologs>>=
  var qc, qp int
  var c, p match
  seg := seg{s:0, l:0, n: make(map[int]bool)}
  rightAnchor := false
#+end_src
#+begin_export latex
We perform the anchor search in a suffix of our query. We update the
\ty{suffix} field of the \ty{query} with a suffix starting at the
current position and ends at the last byte of the query sequence. We
clear the map \ty{segN}, in which we keep segregating sites found in
the current suffix. This map serves as a "buffer" separating the
results of segregation site calling in the current suffix from
\ty{h.N} (Figure~\ref{fig:FHlogic}).

Then we search for an anchor. There are two scenarios: 1) there is a
match that starts right after the last mismatch, 2) there is a distant
match somewhere else in the ESA. There are two functions to be
written.

In the first case, we try to find the longest common prefix (LCP) at
the given positions in query and subject sequences using the function
\ty{lcpAnchor}. In the second case, we turn to the ESA and look for a
matching suffix using the function \ty{esaAnchor}. Both functions
advance in the subject and update the current match \ty{c}. Both
functions return \ty{true} if a significant match is found.

To optimize the search, we put both functions into a short-circuit
evaluation, that is, \ty{esaAnchor} is called only if \ty{lcpAnchor}
has returned \ty{false}~\cite{kloe20:alg}. If a match is found, we
proceed with calculating the end positions of the previous match in
the query and the subject. Then we analyze the current match and
decide whether the current segment can be extended with it. If so, we
extend the current segment. If it cannot be extended and the right
anchor is found, we open a new segment and save the current one in
\ty{h.S}. After these operations, we remember the current match \ty{c}
and jump in the query by at least one nucleotide, which is most likely
to be a mutation.
#+end_export
#+begin_src go <<Anchor search>>=
  for qc < query.l {
	  query.updSuffix(qc)
	  if <<LCP anchor>> || <<ESA anchor>> {
	  p.endQ = qp + p.l
	  p.endS = p.startS + p.l	
	  //<<Analyze current match>>
	  if segCanBeExtended {
		  //<<Extend current segment>>
	  } else {
		  if rightAnchor || p.l / 2 >= subject.a {
			  //<<Close current segment>>
		  }
		  //<<Open a new segment>>
	  }
	  //<<Remember current match>>
  }
	  qc = qc + c.l + 1
  }
  //Close the last segment if open:
  if rightAnchor || p.l / 2 >= subject.a {
	  //<<Close current segment>>
  }
#+end_src
#+begin_export latex
We call \ty{lcpAnchor}, which we will define shortly.
#+end_export
#+begin_src go <<LCP anchor>>=
  lcpAnchor(&c, &p, query, subject, qc, qp)
#+end_src
#+begin_export latex
The function \ty{lcpAnchor} accepts the following inputs:
\begin{enumerate}
  \itemsep0em
  \item a pointer to the current match \ty{c};
  \item a ponter to the previous match \ty{p};
  \item a \ty{query} struct;
  \item a \ty{subject} struct;
  \item current and previous positions in the query.
\end{enumerate}

Regardless of the significance of the match, the function updates the
start and the length of the current match.

First, the function calculates the gap between the current and
previous positions in the query and determines a position in the
subject to attempt matching the longest common prefix (LCP). If this
position is beyond the end of the subject or if the gap is larger than
the minimum anchor length (meaning it's not random), the function
returns \ty{false}. Otherwise, it updates the position in the subject
and finds the LCP length at this position. The length of the current
match is then updated with the found length. If this length meets the
minimum anchor length requirement, the match is considered
significant, and the function returns \ty{true}.
#+end_export
#+begin_src go <<Functions>>=
  func lcpAnchor(c *match, p *match,
	  query query, subject subject,
	  qc, qp int) bool {
	  advance := qc - qp
	  gap := advance - p.l
	  tryS := p.startS + advance
	  if tryS >= subject.totalL || gap > subject.a {                          
		  return false
	  }
	  c.startS = tryS
	  newL := lcpLen(query.l, query.suffix, subject.esa.T[tryS:])
	  c.l = newL
	  return newL >= subject.a
  }
#+end_src
#+begin_export latex
The function \ty{lcpLen} returns the length of the longest common
prefix of two slices of bytes. The auxillary function \ty{min} returns
the smallest of integers.
#+end_export
#+begin_src go <<Functions>>=
  func lcpLen(max int, a, b []byte) int {
      limit := min(len(a), len(b), max)
      count := 0
      for i := 0; i < limit; i++ {
	  if a[i] != b[i] {
	      break
	  }
	  count++
      }
      return count
  }

  func min(vals ...int) int {
      m := vals[0]
      for _, v := range vals {
	  if v < m {
	      m = v
	  }
      }
      return m
  }
#+end_src
#+begin_export latex
We call \ty{esaAnchor} to find a match in the ESA.
#+end_export
#+begin_src go <<ESA anchor>>=
  esaAnchor(&c, query, subject)
#+end_src
#+begin_export latex
The function \ty{esaAnchor} accepts:

\begin{enumerate}
  \itemsep0em
  \item a pointer to the current match \ty{c};
  \item the \ty{query} struct;
  \item the \ty{subject} struct.
\end{enumerate}

The function returns a boolean. Regardless of the significance of the
match, the function updates the start and the length of the current
match.

A match is considered significant if it is unique and not shorter than
the minimum anchor length. A match is unique if it starts and ends at
the same position in the subject's ESA. In terms of a suffix tree, a
unique match ends on a leaf.
#+end_export
#+begin_src go <<Functions>>=
  func esaAnchor(c *match, query query, subject subject) bool {
	  mc := subject.esa.MatchPref(query.suffix)
	  newStartS := subject.esa.Sa[mc.I]
	  newL := mc.L
	  c.startS = newStartS
	  c.l = newL
	  lu := (mc.J == mc.I) && (newL >= subject.a)
	  return lu
  }
#+end_src
#+end_export
#+begin_export latex
We determine if the match:
\begin{itemize}
  \itemsep0em
  \item starts in the subject after the previous match;
  \item is equidistant with the previous match in the subject and
    query;
  \item is located on the same strand in the subject as the previous
    match.
\end{itemize}
If these criteria are met, we label the current segment as
extendible.
#+end_export
#+begin_src go <<Analyze current match>>=
  afterPrev := c.startS > p.endS

  areEquidist := qc - p.endQ == c.startS - p.endS

  onSameStrand := (c.startS < subject.strandL) ==
		  (p.startS < subject.strandL)

  segCanBeExtended := afterPrev &&
		      areEquidist &&
		      onSameStrand
#+end_src
#+begin_export latex
We extend the segment to the end of the new anchor. We calculate the
length of the inter-anchor gap. Then we extend the current segment
with this gap length and the length of the current match. After the
extension, we add new positions to the map of mismatches \ty{segN}
as keys. We also remember that we have just found a new right anchor.
#+end_export
#+begin_src go <<Extend current segment>>=
  prevSegEnd := seg.end()
  gapLen := qc - p.endQ
  seg.l = seg.l + gapLen + c.l
  //<<Add positions to \ty{seg.n}>>
  rightAnchor = true
#+end_src
#+begin_export latex
We extract sequences located between the current pair of anchors from
the subject and the query. Then we compare each nucleotide using the
function \ty{compare}, which we will define shortly. We save the
mismatches' positions to the map \ty{seg.n}. If mismatches are found
on the reverse strand, we project their coordinates onto the forward
strand.
#+end_export
#+begin_src go <<Add positions to \ty{seg.n}>>=
  gapSeqSubject := subject.esa.T[prevSegEnd:prevSegEnd + gapLen]
  gapSeqQuery := query.seq[p.endQ:p.endQ + gapLen]
  for i := 0; i < gapLen; i++ {
	  isSegsite := compare(gapSeqSubject[i], gapSeqQuery[i])
	  ssPos := -1
	  if isSegsite {
		  if prevSegEnd > subject.strandL {
			  ssPos = subject.totalL - prevSegEnd - i - 1 
			  seg.n[ssPos] = true
		  } else {
			  ssPos = prevSegEnd + i
			  seg.n[ssPos] = true
		  }
	  }
  }
#+end_src
#+begin_export latex
The function \ty{compare} compares two bytes and returns \ty{true} if
they are not equal.
#+end_export
#+begin_src go <<Functions>>=
  func compare(s byte, q byte) bool {
	  notEqual := true
	  if s == q {
		  notEqual = false
	  }
	  return notEqual
  }
#+end_src
#+begin_export latex
To close the current segment is to project it onto the forward strand
(if necessary) and append it to the slice of homologies.
#+end_export
#+begin_src go <<Close current segment>>=
  if seg.s > subject.strandL {
	  seg.s = subject.totalL - seg.s - seg.l
  }
  h.S = append(h.S, seg)
#+end_src
#+begin_export latex
To open a segment is to declare its start and length, reset the
segregating site map, and forget that the right anchor was found.
#+end_export
#+begin_src go <<Open a new segment>>=
  seg.s = c.startS
  seg.l = c.l
  seg.n = make(map[int]bool)
  rightAnchor = false
#+end_src
#+begin_export latex
We update the previous position in the query and the previous match.
#+end_export
#+begin_src go <<Remember current match>>=
  qp = qc
  p.l = c.l
  p.startS = c.startS
#+end_src
#+begin_export latex
The last thing we do before \ty{return} is updating \ty{h.N} using the
function \ty{appendKeys} in a loop. The function appends \ty{int} keys
from the map \ty{b} to the map \ty{a}. Both maps hold values of type
\ty{bool}.
#+end_export
#+begin_src go <<Functions>>=
  func appendKeys(a map[int]bool, b map[int]bool) map[int]bool {
	  for key, _ := range(b) {
		  a[key] = true
	  }
	  return a
  }
#+end_src
#+begin_export latex
\subsubsection{Find common homologous regions}
We use the set of homologous regions that we have discovered so far to
identify regions of the subject that are found in a given fraction of
the target genomes. We interpret the threshold fraction (a
\ty{Parameters} field, Section~\ref{Data structure Parameters}), then
we calculate pile heights, that is, how many times a nucleotide of the
subject is covered by piled-up homologous regions. Thereafter, we
convert them to segments, and then get the actual sequences
corresponding to the segments. This task is delegated to the function
\ty{pileHeights}, which is yet to be written.
#+end_export
#+begin_src go <<Find common homologous regions>>=
  //<<Interpret the threshold fraction>>
  p := pileHeights(homologs, subject.strandL)
  //<<Convert pile heights to homologous segments>>
  //<<Convert homologs to sequences>>
#+end_src
#+begin_export latex
We calculate the product of the threshold fraction $f$
(\ty{parameters.Threshold}) and $g$, which is the total number of
genomes being analyzed \textit{excluding} the implied subject. This
number corresponds to the number of files in the \ty{TargetDir} given
that the directory does not contain the reference.

We round the product down to the nearest integer, but if the latter
happens to be zero, we set the value to 1. Thus, we will be always
trying to find the complete or partial intersection for any $0 < f \le
1$ and $g \ge 1$.
#+end_export
#+begin_src go <<Interpret the threshold fraction>>=
  f := parameters.Threshold
  g := numFiles
  t := int(math.Floor(f * float64(g)))
  if t == 0 {
	  t = 1
  }
#+end_src
#+begin_export latex
We import \ty{math}.
#+end_export
#+begin_src go <<Imports>>=
  "math"
#+end_src
#+begin_export latex
\subsubsection{Calculate pile heights}
The function \ty{pileHeights} counts how many times a position in the
subject is covered by homologous segments found in queries.
#+end_export
#+begin_src go <<Functions>>=
  func pileHeights(h Homologs, strandL int) []int {
	  pile := make([]int, strandL)
	  for i := 0; i < len(h.S); i++ {
		  seg := h.S[i]
		  for j := seg.s; j < seg.end(); j++ {
			  pile[j] += 1
		  }
	  }
	  return pile
  }
#+end_src
#+begin_export latex
\subsubsection{Convert the pile heigths to homologous segments}
Once we have pile heights at hand, we can convert them to actual
segments of the reference. If the reference has been cleaned, two
adjacent homologous segments can be merged into one. The cleaning
implies removal of non-canonical nucleotides, that is, keeping only
\ty{A, T, G, C}. The removal shifts proper nucleotides that followed
the removed characters to the left. As the result, two regions
separated in the original sequence become adjacent in the cleaned
sequence (Figure~\ref{fig:MatchEnds}, A). This creates a risk of an
incorrect interpretation of pile heights unless match ends are not
neglected (Figure~\ref{fig:MatchEnds}, B).

\begin{figure}[H]
    \includegraphics[width=\linewidth]{figMatchEnds.eps}
    \caption{Normalization of the subject affects how pile heights are
      interpreted. In panel A, a region of the real SARS-CoV-2 genome
      from \ty{data/i/sars/t1.fasta} is used as a subject (reference)
      to find matches in 4 queries. This region contains 20
      undetermined nucleotides (\ty{N}), which are removed during
      normalization. The right fragment then gets a new start
      coordinate (shown in red). In panel B, the normalized sequence
      shows pile heights of 4 (grey), indicating that these
      nucleotides are present at the same positions in all 4
      queries. The red line marks the right border of the segment
      ending with \ty{TTTGT}. If the pile heights were converted to
      the final output without considering this border, they would
      produce a chimeric sequence, which does not exist in the
      reference.}
    \label{fig:MatchEnds}
\end{figure}

To avoid chimeric sequences in the output, we create 'a map of
adjacency'. This map holds ends of the left segments from pairs. The
creation of this map is delegated to the \ty{makeMapAdj} function,
which is not written so far. If we haven't opted for cleaning the
subject, we just proceed with an empty map without calling the
\ty{makeMapAdj}. Once the map is built. we perform the conversion and
return the segments that make up the intersection.
#+end_export
#+begin_src go <<Convert pile heights to homologous segments>>=
  isAdj := make(map[int]bool)
  if parameters.CleanQuery {
	  isAdj = makeMapAdj(homologs)
  }
  intersection := pileToSeg(p, t, isAdj)
  homologs.S = intersection
#+end_src
#+begin_export latex
We define the function \ty{makeMapAdj}. We search for such pairs of
segments, where the end of the left segment is immideately followed by
the start of the right segment. In terms of end-exclusive coordinates,
this implies that the start and the end share the same coordinate.

We declare two auxullary variables, \ty{starts} (a map of booleans
with integer keys) and \ty{ends} (a slice of integers). Then we use
them to find ends that coincide with at least one start. We save such
ends in the adjacency map.
#+end_export
#+begin_src go <<Functions>>=
  func makeMapAdj(h Homologs) map[int]bool {
	  //<<Describe starts and ends>>
	  isAdj := make(map[int]bool)
	  for _, e := range ends {
		  if starts[e] {
			  isAdj[e] = true
		  }
	  }
	  return isAdj
  }
#+end_src
#+begin_export latex
We traverse the slice of segments, saving each segment's start to the
map \ty{starts} and each end to the slice of \ty{ends}.
#+end_export
#+begin_src go <<Describe starts and ends>>=
  starts := make(map[int]bool)
  ends := []int{}
  for i := 0; i < len(h.S); i++ {
	  seg := h.S[i]
	  starts[seg.s] = true
	  ends = append(ends, seg.end())
  }
#+end_src
#+begin_export latex
The function \ty{pileToSeg} returns a slice of segments, where each
position is covered at least a given number of times. We iterate over
keys and values of the pile.  We close an opened segment if the pile
height is below the threshold, otherwise we extend the segment. If
there is no opened segment, we open it. In the end we close the last
segment and return the result.
#+end_export
#+begin_src go <<Functions>>=
  func pileToSeg(p []int, t int, isAdj map[int]bool) []seg {
	  var segs []seg
	  var seg seg
	  segIsOpen := false
	  for k, v := range p {
		  if segIsOpen {
			  if v < t {
				  //<<Close intersection segment>>
			  } else {
				  //<<Extend intersection segment>>
			  }
		  } else {
			  //<<New intersection segment>>
		  }
	  }
	  if segIsOpen {
		  segs = append(segs, seg)
	  }
	  return segs
  }
#+end_src
#+begin_export latex
To close a segment is to append it to the resulting slice and toggle
segment's status.
#+end_export
#+begin_src go <<Close intersection segment>>=
  segs = append(segs, seg)
  segIsOpen = false
#+end_src
#+begin_export latex
The extension implies adding one to the segment's length. After the
extension we check whether the next position is listed in the
adjacency map. If so, we close the segment from here.
#+end_export
#+begin_src go <<Extend intersection segment>>=
  seg.l += 1
  if isAdj[k+1] {
	  //<<Close intersection segment>>
  }
#+end_src
#+begin_export latex
We open a new segment if the pile height is not less than the
threshold.
#+end_export
#+begin_src go <<New intersection segment>>=
  if v >= t {
	  seg.s = k
	  seg.l = 1
	  segIsOpen = true
  }
#+end_src
#+begin_export latex
\subsubsection{Convert the homologs to sequences}
We retrieve the formatting switches from the \ty{parameters} struct
and call the function \ty{homologsToFasta}, which is yet to be
written. The output of this function is the \ty{return} of
\ty{Intersect} (Section~\ref{Intersect})!
#+end_export
#+begin_src go <<Convert homologs to sequences>>=
  printN := parameters.PrintN
  printOneBased := parameters.PrintOneBased
  printSegSitePos := parameters.PrintSegSitePos
  result := homologsToFasta(homologs, subject, printN,
	  printOneBased, printSegSitePos, shiftRefRight)
  return result
#+end_src
#+begin_export latex
The function \ty{homologsToFasta()} accepts a struct of \ty{Homologs},
a struct of \ty{subject}, and \ty{bool} switches to format the output
sequence and headers. The function returns a slice of pointers to
fasta entries (type \ty{fasta.Sequence}).

We initialize the output slice, then convert each segment of the input
\ty{Homologs} into a \ty{fasta.Sequence}. For this, we construct its
data, its header, and use them to make a sequence.
#+end_export
#+begin_src go <<Functions>>=
  func homologsToFasta(h Homologs, subject subject,
	  printN bool,
	  printOneBased bool,
	  printSegSitePos bool,
	  shiftRefRight bool) []*fasta.Sequence {
	
	  var sequences []*fasta.Sequence
	  segs := h.S
	  ns := h.N
	  for _, seg := range segs {
		  //<<Construct sequence data>>
		  //<<Construct sequence header>>
		  seq := fasta.NewSequence(header, data)
		  sequences = append(sequences, seq)		
	  }
	  return sequences
  }
#+end_src
#+begin_export latex
As the majority of bytes are not \ty{N}s, we copy the part of the
subject's sequence directly, and only then check if we need to print
\ty{N}s.

We get start and end positions of the current segment, initialize
a slice of bytes, and append bytes to it.
#+end_export
#+begin_src go <<Construct sequence data>>=
  start := seg.s
  end := seg.end()
  data := make([]byte, seg.l)
  copy(data, subject.esa.T[start:end])
  if printN {
	  //<<Replace segregating sites with \ty{N}s>>
  }
#+end_src
#+begin_export latex
We check if each position of the segment is listed in the map of
segregating sites. If so, we replace the byte with an \ty{N}.
#+end_export
#+begin_src go <<Replace segregating sites with \ty{N}s>>=
  for j := 0; j < seg.l; j++ {
	  if ns[start+j] {
		  data[j] = 'N'
	  }
  }
#+end_src
#+begin_export latex
We construct the sequence's header that contains the orginal contig's
header \ty{ch}, an underscore, and coordianates of the start (\ty{cs}) and the
end (\ty{ce}):
\begin{verbatim}
ch_(cs..ce)
\end{verbatim}
To find \ty{ch}, \ty{cs}, and \ty{ce}, we call \ty{findSegment}, which
we still have to implement. We convert zero-based end-exclusive
coordinates to one-based \textit{end-inclusive} coordinates if the
respective switch is set to \ty{true}. To do this, we add one to the
start.

If \ty{printSegSitePos} has been switched to \ty{true}, we build
segsite positions string with the \ty{buildSegSiteStr} function, which
is yet to be defined. Then we append the string to the header.
#+end_export
#+begin_src go <<Construct sequence header>>=
  ch, cs, ce := findSegment(seg, subject, shiftRefRight)
  if printOneBased {
	  cs += 1
  }
  header := fmt.Sprintf("%s_(%d..%d)", ch, cs, ce)
  if printSegSitePos {
	  segsites := buildSegSiteStr(seg, ns, printOneBased)
	  header += " " + segsites
  }
#+end_src
#+begin_export latex
\subsubsection{Auxillary functions for output formatting}
\textbf{\ty{findSegment}} accepts a segment, a struct of \ty{subject},
and a switch for processing shift fields. It finds the subject
contig, to which the segment belongs, and returns its header without
the shift fields and the coordinates of the segment on the contig.

We initialize the variables:
\begin{itemize}
  \itemsep0em
\item \ty{ch} for contig header,
\item \ty{cs}, \ty{ce} for start and end of the segment on the related
  contig,
  \item a \ty{shift} value to add to \ty{cs}. It takes a non-zero
    value if \ty{shiftRefRight} is toggled.
\end{itemize}

After this, we 'unpack' some fields of the \ty{subject}. Then we check
whether our segment is located within some of the contigs using the
function \ty{isWithin}, which we still need to write. If so, we get
the contig's header and find the shift value using the map of
shifts. After this, we remove the shifr fields.

Once we have the shift value at hand, we can calculate \ty{cs} and
\ty{ce}. A segment can be found only once because the homologs are
unique and do not overlap, so we break as soon as we've found the
contig it belongs to.
#+end_export
#+begin_src go <<Functions>>=
  func findSegment(seg seg, subject subject,
	  shiftRefRight bool) (string, int, int) {

	  var ch string
	  var cs, ce, shift int
	  contigHeaders := subject.contigHeaders
	  contigSegments := subject.contigSegments
	  contigShifts := subject.contigShifts

	  for i, contigSeg := range contigSegments {
		  if isWithin(seg, contigSeg) {
			  ch = contigHeaders[i]
			  if shiftRefRight {
				  shift = contigShifts[ch]
				  //<<Truncate the shift field>>
			  }
			  cs = seg.s - contigSeg.s + shift
			  ce = cs + seg.l
			  break
		  }
	  }
	  return ch, cs, ce
  }
#+end_src
#+begin_export latex
\textbf{\ty{isWithin}} checks if an inner \ty{seg} is located within
an outer \ty{seg}.
#+end_export
#+begin_src go <<Functions>>=
  func isWithin(in seg, out seg) bool {
	  return in.s >= out.s && in.end() <= out.end()
  }
#+end_src
#+begin_export latex
We remove the remainders of the \ty{\$}-separated fields from the
contig name.
#+end_export
#+begin_src go <<Truncate the shift field>>= 
  chFields := strings.Split(ch, "$")
  ch = chFields[0]
#+end_src
#+begin_export latex
\textbf{\ty{buildSegSiteStr}} accepts a segment, a map of segregating
sites (\ty{Homologs.N}), and a switch for printing one-based
coordinates. It builds a string of segregating site
coordinates. Imagine a homologous region that has substitutions at
positions 2, 4, 6, 7, 8, 10 (6 in total). The \ty{segSiteStr} of this
sequence has the following format:
\begin{verbatim}
6 2 4 6 7 8 10
\end{verbatim}
 We calculate the number of segregating sites and begin to build our
 \ty{segSiteStr}. We report only sites found within the specificed
 segment.
#+end_export
#+begin_src go <<Functions>>=
  func buildSegSiteStr(seg seg, ns map[int]bool,
	  printOneBased bool) string {
	  var positions []int
	  for i := seg.s; i < seg.end(); i++ {
		  //<<Add positions found within the \ty{seg}>>
	  }
	  //<<Build the segsite string>>
  }
#+end_src
#+begin_export latex
We traverse the segregating sites of the current segment, transform
the coordinates, and save the coordinate in \ty{positions}.
#+end_export
#+begin_src go <<Add positions found within the \ty{seg}>>=
  if ns[i] {
	  pos := i - seg.s
	  //<<Convert to one-based?>>
	  positions = append(positions, pos)
  }
#+end_src
#+begin_export latex
We add one if the corresponding \ty{Parameters}' switch is toggled.
#+end_export
#+begin_src go <<Convert to one-based?>>=
  if printOneBased {
	  pos++
  }
#+end_src
#+begin_export latex
We initialize a \ty{Builder} for our output string. Then we write the
number of segregating sites followed by their coordinates separated with
blanks.
#+end_export
#+begin_src go <<Build the segsite string>>=
  var builder strings.Builder
  numSegSites := len(positions)
  builder.WriteString(strconv.Itoa(numSegSites))

  for _, pos := range positions {
	  builder.WriteString(" ")
	  builder.WriteString(strconv.Itoa(pos))
  }

  return builder.String()
#+end_src
