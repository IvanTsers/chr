#+begin_export latex
\section{Implementation}
The package \ty{chr} has hooks for imports, data structures, methods,
and functions.
#+end_export
#+begin_src go <<chr.go>>=
  package chr
  import (
	  //<<Imports>>
  )
  //<<Data structures>>
  //<<Methods>>
  //<<Functions>>
#+end_src
#+begin_export latex
\subsection{\textbf{Data structure \ty{Homologs}}}
!Data structure \ty{Homologs} describes homologous regions of a
!subject found in queries. This data type contains a slice of segments
!of the subject \ty{S} and a map of segregation sites \ty{N}.

The data structure \ty{seg} contains a start \ty{s} and a length
\ty{l} of a segment, as defined in Section~\ref{Data structure seg}.
#+end_export
#+begin_src go <<Data structures>>=
  type Homologs struct {
	  S []seg
	  N map[int]bool
  }
#+end_src
#+begin_export latex
\subsubsection{Method \ty{sort}}
The method \ty{sort} orders \ty{Homologs} by their start positions.
#+end_export
#+begin_src go <<Functions>>=
  func (h *Homologs) sort() *Homologs {
	  sort.Slice(h.S, func(i, j int) bool {
		  return h.S[i].s < h.S[j].s
	  })
	  return h
  }
#+end_src
#+begin_export latex
We import \ty{sort}.
#+end_export
#+begin_src go <<Imports>>=
  "sort"
#+end_src
#+begin_export latex
\subsubsection{Method \ty{reduceOverlaps}}
This method reduces a slice of overlapping homologous regions
\ty{Homologs.S} to a slice of non-overlapping homologous regions of
the longest total length.

If slice \ty{Homologs.S} does not contain at least two elements, we
return early. If it does, we sort it and reduce overlapping stacks of
segments to the longest chain of co-linear non-overlapping segments
using the algorithm for two-dimensional fragment
chaining~\cite{ohl13:alg} as implemented in
\ty{phylonium}~\cite{kloe20:alg}. We initialize variables, then we
calculate chain scores, find links of the longest chain through
backtracking, and return the chain.
#+end_export
#+begin_src go <<Methods>>=
  func (h *Homologs) reduceOverlaps() {
	  slen := len(h.S)
	  if slen < 2 {
		  return
	  }
	  h.sort()
	  segs := h.S
	  //<<Initialize chaining>>
	  //<<Calculate chain score>>
	  //<<Backtrack the chain>>
	  //<<Return the chain>>
  }
#+end_src
#+begin_export latex
We declare variables describing the chain. For each element of
\ty{segs}, we initialize:
\begin{itemize}
  \itemsep0em
  \item \ty{predecessor}---the previous segment in the longest chain
    ending before \ty{segs[i]});
  \item \ty{score}---the length of the longest chain ending at
    \ty{segs[i]}. The initial score of the chain is the first
    segment's length;
  \item \ty{visited}---whether \ty{segs[i]} has been visited during
    backtracking of chain links.
\end{itemize}
We also initialize the first elements for \ty{score} and
\ty{predecessor}.
#+end_export
#+begin_src go <<Initialize chaining>>=
  predecessor := make([]int, slen)
  score := make([]int, slen)
  visited := make([]bool, slen)
  score[0] = segs[0].l
  predecessor[0] = -1
#+end_src
#+begin_export latex
We calculate the chain score to maximize the number of non-overlapping
segments in \ty{segs}. Starting from the second segment, we traverse
each segment starting from the second element. For each \ty{segs[i]},
we find its preceding segment \ty{segs[k]} that can form the longest
chain ending before \ty{segs[i]} starts.
#+end_export
#+begin_src go <<Calculate chain score>>=
  for i := 1; i < slen; i++ {
	  maxScore := 0
	  maxIndex := -1
	  for k := 0; k < i; k++ {
		  if segs[k].end() <= segs[i].s {
			  if score[k] > maxScore {
				  maxScore = score[k]
				  maxIndex = k
			  }
		  }
	  }
	  predecessor[i] = maxIndex
	  if maxIndex != -1 {
	      score[i] = segs[i].l + score[maxIndex]
	  } else {
	      score[i] = segs[i].l
	  }
  }
#+end_src
#+begin_export latex
We are to find \ty{s}, which is the index of the highest-score segment
in \texttt{segs}. This segment is the final link in the chain we're
looking for, and its score is the total score of the chain. To find
\ty{s}, we will use the \ty{argmax} function, which we will define
shortly. Once we identify \ty{s}, we go through \ty{predecessor} links
and mark the segments in \ty{visited}.
#+end_export
#+begin_src go <<Backtrack the chain>>=
  s := argmax(score)
  for s != -1 {
	  visited[s] = true
	  s = predecessor[s]
  }
#+end_src
#+begin_export latex
The function \ty{argmax} returns index of the maximum value in the
input slice of integers.
#+end_export
#+begin_src go <<Functions>>=
  func argmax(x []int) int {
      maxIdx := 0
      for i := 1; i < len(x); i++ {
	  if x[i] > x[maxIdx] {
	      maxIdx = i
	  }
      }
      return maxIdx
  }
#+end_src
#+begin_export latex
We extract only visited elements of \ty{segs} and set the new value of
\ty{h.S}.
#+end_export
#+begin_src go <<Return the chain>>=
  var segred []seg
  for i := 0; i < slen; i++ {
	  if visited[i] {
		  segred = append(segred, segs[i])
	  }
  }
  h.S = segred
#+end_src
#+begin_export latex
\subsection{Data structure \ty{seg}} \label{Data structure seg}
We declare a custom data struct \ty{seg} to store segments of the
forward strand of the subject. A \ty{seg} contains a zero-based start
and length of a region of a subject sequence.
#+end_export
#+begin_src go <<Data structures>>=
  type seg struct {
	  s int
	  l int
  }
#+end_src
#+begin_export latex
\subsubsection{Method \ty{end}}
The method \ty{end()} returns an inclusive coordinate of the end of a
\ty{seg}. Thus, the package operates with zero-based end-inclusive
coordinates.
#+end_export
#+begin_src go <<Methods>>=
  func (seg *seg) end() int {
	  return seg.s + seg.l
  }
#+end_src
#+begin_export latex
\subsubsection{Method \ty{newSeg}}
Constructor method \ty{newSeg()} returns a segment of specified start
and length.
#+end_export
#+begin_src go <<Methods>>=
  func newSeg(x, y int) seg {
	  return seg{s:x, l:y}
  }
#+end_src
#+begin_export latex
\subsection{Data structure \ty{subject}} \label{Data structure subject}
This data structure describes a subject sequence, that is, a
reference, to which query sequences are compared. A \ty{subject}
contains:
\begin{itemize}
  \itemsep0em
  \item an enhanced suffix array (ESA), as described in the package
    \ty{esa}; note that an \ty{esa.Esa} structure has the filed \ty{T}
    holding the original sequence;
  \item a total length of the sequence;
  \item a length of the forward strand of the sequence;
  \item the minimum anchor length for the sequence;
  \item headers of contigs used to build the ESA;
  \item coordinates of the contigs on the \ty{Esa.T}.
\end{itemize}
#+end_export
#+begin_src go <<Data structures>>=
  type subject struct {
	  esa *esa.Esa
	  totalL int
	  strandL int
	  a int
	  contigHeaders []string
	  contigSegments []seg
  }
#+end_src
#+begin_export latex
We import \ty{esa}.
#+end_export
#+begin_src go <<Imports>>=
  "github.com/evolbioinf/esa"
#+end_src
#+begin_export latex
\subsection{Data structure \ty{query}} \label{Data structure query}
The data structure \ty{query} describes a query sequence that is being
compared to the subject. A \ty{query} contains:
\begin{itemize}
  \itemsep0em
  \item the sequence as a slice of bytes;
  \item the total length of the sequence;
  \item a suffix of the sequence.
\end{itemize}
#+end_export
#+begin_src go <<Data structures>>=
  type query struct {
	  seq []byte
	  l int
	  suffix []byte
  }
#+end_src
#+begin_export latex
\subsubsection{Method \ty{updSuffix}}
This method updates the \ty{suffix} field with a prefix starting at
the position \ty{x}.
#+end_export
#+begin_src go <<Methods>>=
  func (q *query) updSuffix(x int) {
	  q.suffix = q.seq[x:]
  }
#+end_src
#+begin_export latex
\subsection{Data structure \ty{match}}
This custom data structure describes an exact match. A \ty{match}
contains:
\begin{itemize}
  \itemsep0em
  \item its length;
  \item starting positions in subject and query sequences;
  \item ending positions in subject and query sequences;
\end{itemize}
#+end_export
#+begin_src go <<Data structures>>=
  type match struct {
	  l int
	  startS int
	  startQ int
	  endS int
	  endQ int
  }
#+end_src
#+begin_export latex
\subsection{\textbf{Data structure \ty{Parameters}}} \label{Data structure Parameters}
!Fields of this data structure contain parameters used to call
!\ty{Intersect()}. The parameters include:

!1) a reference;
!2) path to the directory of target genomes \textit{minus the reference};
!3) threshold, the minimum fraction of intersecting genomes;
!4) p-value of the shustring length (needed for \ty{sus.Quantile});
!5) a switch to clean* subject's sequence;
!6) a switch to clean* query's sequences;
!7) a switch to print positions of segregation sites in output's headers;
!8) a switch to print \ty{N} at the positions of mismatches;
!8) a switch to print one-based coordinates.
!*To clean a sequence is to remove non-ATGC nucleotides.

\ty{Intersect} is defined in Section~\ref{Intersect}.
#+end_export
#+begin_src go <<Data structures>>=
  type Parameters struct{
	  Reference []*fasta.Sequence
	  TargetDir string
	  Threshold float64
	  ShustrPval float64
	  CleanSubject bool
	  CleanQuery bool
	  PrintSegSitePos bool
	  PrintN bool
	  PrintOneBased bool
  }
#+end_src
#+begin_export latex
We import \ty{fasta}.
#+end_export
#+begin_src go <<Imports>>=
  "github.com/ivantsers/fasta"
#+end_src
#+begin_export latex
\subsection{\textbf{Function \ty{Intersect}}} \label{Intersect}
!The function \ty{Intersect} accepts a struct of \ty{Parameters} and
!returns sequences of homologous regions, common for subject
!(reference) and query sequences.

The data structure \ty{Parameters} is defined in Section~\ref{Data
  structure Parameters}. We "unpack" some fields of the
\ty{Parameters} to make short aliases and set default values for some
fields. Then we count files in the specified target directory. If
there are none, we don't have the material to intersect with, thus we
return the original sequence. If the files do exist, we prepare the
subject from the reference and find common homologous regions.
#+end_export
#+begin_src go <<Functions>>=
  func Intersect(parameters Parameters) []*fasta.Sequence {
	  r := parameters.Reference
	  d := parameters.TargetDir
	  //<<Set default parameters>>
	  numFiles := 0
	  //<<Count files in \ty{d/}>>
	  if numFiles == 0 {
		  return r
	  } else {
		  //<<Prepare the \ty{subject}>>
		  //<<Find homologous regions>>
		  //<<Find common homologous regions>>
	  }
  }
#+end_src
#+begin_export latex
If the \ty{ShustrPval} field has not been set by the user, we set it
to the default \ty{0.95}.
#+end_export
#+begin_src go <<Set default parameters>>=
  if parameters.ShustrPval == 0.0 {
	  parameters.ShustrPval = 0.95
  }
#+end_src
#+begin_export latex
We open the directory and check for an error. If there is one, we
report it and stop. If there is no error, we count files in the target
directory.
#+end_export
#+begin_src go <<Count files in \ty{d/}>>=
  dirEntries, err := os.ReadDir(d)
  if err != nil {
	  fmt.Fprintf(os.Stderr,
		  "chr.Intersect: error reading %v: %v", d, err)
	  os.Exit(1)
  }
  numFiles = len(dirEntries)
#+end_src
#+begin_export latex
We import \ty{os} and \ty{fmt}.
#+end_export
#+begin_src go <<Imports>>=
  "os"
  "fmt"
#+end_src
#+begin_export latex
\subsubsection{Prepare the subject}
The preparation of the subject implies the following steps:
\begin{itemize}
  \itemsep0em
  \item initialize a \ty{subject} struct (Section~\ref{Data structure
    subject});
  \item normalize the subject's contigs;
  \item process the subject's contigs';
  \item calculate the minimum anchor length;
  \item build an ESA;
  \item populate the \ty{subject} struct.
\end{itemize}
#+end_export
#+begin_src go <<Prepare the \ty{subject}>>=
  var subject subject
  //<<Normalize the contigs>>
  //<<Process the contigs>>
  //<<Calculate the minimum anchor length>>
  //<<Build subject's ESA>>
  //<<Populate \ty{subject}>>
#+end_src
#+begin_export latex
We normalize the contigs. The complete normalization implies 1)
converting nucleotides to uppercase, and 2) removing non-canonical
nucleotides. We do the latter only if \ty{parameters.CleanSubject} has
been set to \ty{true}.
#+end_export
#+begin_src go <<Normalize the contigs>>=
  for i, _ := range r {
	  if parameters.CleanSubject {
		  r[i].Clean()
	  }
	  r[i].DataToUpper()
  }
#+end_src
#+begin_export latex
We extract the first subject sequence's \ty{Header} and
\ty{Data}. Then we use them to initialize the fields describing the
subject's contigs. Most likely, there will be more than one contig, so
we concatenate them.
#+end_export
#+begin_src go <<Process the contigs>>=
  subjectHeader := r[0].Header()
  subjectData := r[0].Data()
  contigHeaders := []string{subjectHeader}
  contigSegs := []seg{newSeg(0, len(subjectData))}
  cL := len(subjectData)
  if len(r) > 1 {
	  //<<Concatenate subject's contigs>>
  }
#+end_src
#+begin_export latex
We traverse the slice of subject sequences, extract their headers and
data. We append the header to the slice of contig
headers. Then we append an exclamation mark to the combined subject
data and increase the total length by one. The new total length gives
us the start of the contig on the concatenated subject data. We create
a \ty{seg} and append it to the slice of contig segments. Finally, we
append the contig's data to the combined subject's data and increase
the total length.
#+end_export
#+begin_src go <<Concatenate subject's contigs>>=
  for i := 1; i < len(r); i++ {
	  seq := r[i]
	  seqH := seq.Header()
	  seqD := seq.Data()
	  seqL := len(seqD)
	  contigHeaders = append(contigHeaders, seqH)
	  subjectData = append(subjectData, '!')
	  cL += 1
	  cseg := newSeg(cL, seqL)
	  contigSegs = append(contigSegs, cseg)
	  subjectData = append(subjectData, seqD...)
	  cL += seqL
  }
#+end_src
#+begin_export latex
We calculate GC content of the subject and use it to find the length
of a non-random shustring. The latter task is delegated to
\ty{sus.Quantile}, which we use with the shustring p-value specified
in \ty{parameters.ShustrPval} that we also refer to as \ty{pval}.
#+end_export
#+begin_src go <<Calculate the minimum anchor length>>=
  atgc := 0.0
  gc := 0.0
  for _, c := range subjectData {
	  if c == 'A' || c == 'C' || c == 'G' || c == 'T' {
		  atgc++
		    if c == 'C' || c == 'G' {
			    gc++
		    }
	  }
  }
  gcContent := gc/atgc
  pval := parameters.ShustrPval
  minAncLen := sus.Quantile(cL, gcContent, pval)
#+end_src
#+begin_export latex
We import \ty{sus}.
#+end_export
#+begin_src go <<Imports>>=
  "github.com/evolbioinf/sus"
#+end_src
#+begin_export latex
We calculate the reverse strand, append it to the subject, and build
an ESA.
#+end_export
#+begin_src go <<Build subject's ESA>>=
  rev := fasta.NewSequence("reverse", subjectData)
  rev.ReverseComplement()
  subjectData = append(subjectData, rev.Data()...)
  sa := esa.MakeEsa(subjectData)
#+end_src
#+begin_export latex
We populate the \ty{subject} struct with the calculated values.
#+end_export
#+begin_src go <<Populate \ty{subject}>>=
  subject.esa = sa
  subject.totalL = len(subjectData)
  subject.strandL = len(subjectData)/2
  subject.a = minAncLen
  subject.contigHeaders = contigHeaders
  subject.contigSegments = contigSegs
#+end_src
#+begin_export latex
\subsubsection{Find homologous regions}
We initialize a variable of type \ty{Homologs} to store found
homologous regions. We iterate over the files, prepare \ty{queries},
and call the \ty{findHomologs} function, which we still have to
write. We append the output of this function to the \ty{Homologs}.
#+end_export
#+begin_src go <<Find homologous regions>>=
  homologs := Homologs{S: []seg{}, N: make(map[int]bool)}
  for _, entry := range dirEntries {
	  //<<Prepare a \ty{query}>>
	  h := findHomologs(query, subject)
	  homologs.S = append(homologs.S, h.S...)
	  for pos, _ := range h.N {
		  homologs.N[pos] = true
	  }
  }
#+end_src
#+begin_export latex
We initialize and populate a \ty{query} struct (Section~\ref{Data
  structure query}). We build a path to a query file, read all
sequences from it, concatenate and normalize them (clean when asked,
convert to uppercase always). Then we set the values for the \ty{seq}
and \ty{l} fields.
#+end_export
#+begin_src go <<Prepare a \ty{query}>>=
  var query query
  filePath := d + "/" + entry.Name()
  f, _ := os.Open(filePath)
  queryData := fasta.ReadAll(f)
  f.Close()
  qSeq := fasta.Concatenate(queryData, 0)
  if parameters.CleanQuery {
	  qSeq.Clean()
  }
  qSeq.DataToUpper()
  query.seq = qSeq.Data()
  query.l = qSeq.Length()
#+end_src
#+begin_export latex
The function \ty{findHomologs} accepts structs of \ty{subject} and
\ty{query}. The function returns a \ty{Homologs} struct.

We initialize variables to operate with during the search of
homologies, conduct the search, reduce overlaps, and return the
results. If no homologs were found, we return an empty \ty{Homologs}
slice, which is still a valid result.
#+end_export
#+begin_src go <<Functions>>=
  func findHomologs(query query, subject subject) Homologs { 
	  h := Homologs{S: []seg{}, N: make(map[int]bool)}
	  //<<Initialize the search of homologs>>
	  //<<Anchor search>>
	  h.reduceOverlaps()
	  return h
  }
#+end_src
#+begin_export latex
We declare variables to operate with during the search. These are:
\begin{itemize}
  \itemsep0em
  \item current and previous positions in the query;
  \item current and previous \ty{matches}:
  \item a \ty{segment};
  \item an indicator of the right anchor;
\end{itemize}
#+end_export
#+begin_src go <<Initialize the search of homologs>>=
  var qc, qp int
  var c, p match
  var seg seg
  rightAnchor := false
#+end_src
#+begin_export latex
We perform the anchor search for each suffix in our query. We update
the \ty{suffix} field of the \ty{query} with a suffix starting at the
current position and ends at the last byte of the query sequence. Then
we search for an anchor. There are two scenarios: 1) there is a match
that starts right after the last mismatch, 2) there is a 'remote'
match somewhere else in the ESA. There are two functions to be
written.

In the first case, we try to find the longest common prefix (LCP) at
the given positions in query and subject sequences using the function
\ty{lcpAnchor}. In the second case, we turn to the ESA and look for a
matching suffix using the function \ty{esaAnchor}. Both functions
advance in the subject and update the current match \ty{c}. Both
functions return \ty{true} if a significant match is found.

To optimize the search, we put both functions into a short-circuit
evaluation, that is, \ty{esaAnchor} is called only if \ty{lcpAnchor}
has returned \ty{false}~\cite{kloe20:alg}. If a match is found, we
proceed with calculating the end positions of the previous match in
the query and the subject. Then we analyze the current match and
decide whether the current segment can be extended with it. If so, we
extend the current segment. If it cannot be extended and the right
anchor is found, we open a new segment and save the current one in
\ty{h.S}. After these operations, we remember the current match \ty{c}
and jump in the query by at least one nucleotide, which is most likely
to be a mutation.
#+end_export
#+begin_src go <<Anchor search>>=
  for qc < query.l {
	  query.updSuffix(qc)
	  if <<LCP anchor>> || <<ESA anchor>> {
	  p.endQ = qp + p.l
	  p.endS = p.startS + p.l	
	  //<<Analyze current match>>
	  if segCanBeExtended {
		  //<<Extend current segment>>
	  } else {
		  if rightAnchor || p.l / 2 >= subject.a {
			  //<<Close current segment>>
		  }
		  //<<Open a new segment>>
	  }
	  //<<Remember current match>>
  }
	  qc = qc + c.l + 1
  }
  //Close the last segment if open:
  if rightAnchor || p.l / 2 >= subject.a {
	  //<<Close current segment>>
  }
#+end_src
#+begin_export latex
We call \ty{lcpAnchor}, which we will define shortly.
#+end_export
#+begin_src go <<LCP anchor>>=
  lcpAnchor(&c, &p, query, subject, qc, qp)
#+end_src
#+begin_export latex
The function \ty{lcpAnchor} accepts the following inputs: 1) a pointer
to the current match \ty{c}; 2) a ponter to the previous match \ty{p};
3) a \ty{query} struct; 4) a \ty{subject} struct; 5) current and
previous positions in the query.  Regardless of the significance of
the match, the function updates the start and the length of the
current match.

First, the function calculates the gap between the current and
previous positions in the query, and determines a position in the
subject to attempt matching the longest common prefix (LCP). If this
position is beyond the end of the subject or if the gap is larger than
the minimum anchor length (meaning it's not random), the function
returns \ty{false}. Otherwise, it updates the position in the subject
and finds the LCP length at this position. The length of the current
match is then updated with the found length. If this length meets the
minimum anchor length requirement, the match is considered
significant, and the function returns \ty{true}.
#+end_export
#+begin_src go <<Functions>>=
  func lcpAnchor(c *match, p *match,
	  query query, subject subject,
	  qc, qp int) bool {
	  advance := qc - qp
	  gap := advance - p.l
	  tryS := p.startS + advance
	  if tryS >= subject.totalL || gap > subject.a {                          
		  return false
	  }
	  c.startS = tryS
	  newL := lcp(query.l, query.suffix, subject.esa.T[tryS:])
	  c.l = newL	
	  return newL >= subject.a
  }
#+end_src
#+begin_export latex
The function \ty{lcp} returns the length of the longest common prefix
of two slices of bytes.
#+end_export
#+begin_src go <<Functions>>=
  func lcp(max int, a, b []byte) int {
      count := 0
      for i := 0; i < max; i++ {
	  if i >= len(a) || i >= len(b) || a[i] != b[i] {
	      break
	  }
	  count++
      }
      return count
  }
#+end_src
#+begin_export latex
We call \ty{esaAnchor} to find a match in the ESA.
#+end_export
#+begin_src go <<ESA anchor>>=
  esaAnchor(&c, query, subject)
#+end_src
#+begin_export latex
The function \ty{esaAnchor} accepts: 1) a pointer to the current match
\ty{c}; 2) the \ty{query} struct; 3) the \ty{subject} struct. The
function returns a boolean. Regardless of the significance of the
match, the function updates the start and the length of the current
match.

A match is considered significant if it is unique and not shorter than
the minimum anchor length. A match is unique if it starts and ends at
the same position in the subject's ESA. In terms of a suffix tree, a
unique match ends on a leaf.
#+end_export
#+begin_src go <<Functions>>=
  func esaAnchor(c *match, query query, subject subject) bool {
	  mc := subject.esa.MatchPref(query.suffix)
	  newStartS := subject.esa.Sa[mc.I]
	  newL := mc.L
	  c.startS = newStartS
	  c.l = newL
	  lu := (mc.J == mc.I) && (newL >= subject.a)
	  return lu
  }
#+end_src
#+end_export
#+begin_export latex
We analyze if the match:
\begin{itemize}
  \itemsep0em
  \item starts in the subject after previous match;
  \item is equidistant with previous match in the subject and
    query;
  \item is located on the same strand in the subject as previous
    match.
\end{itemize}
If these criteria are met, we qualify the current segment as
extendible.
#+end_export
#+begin_src go <<Analyze current match>>=
  afterPrev := c.startS > p.endS

  areEquidist := qc - p.endQ == c.startS - p.endS

  onSameStrand := (c.startS < subject.strandL) ==
		  (p.startS < subject.strandL)

  segCanBeExtended := afterPrev &&
		      areEquidist &&
		      onSameStrand
#+end_src
#+begin_export latex
We extend the segment to the end of the new anchor. We calculate the
length of the inter-anchor gap. Then we extend the current segment
with this gap length and the length of the current match. After the
extension, we add new positions to the map of mismatches \ty{h.N} as
keys. We also remember that we have just found a new right anchor.
#+end_export
#+begin_src go <<Extend current segment>>=
  prevSegEnd := seg.end()
  gapLen := qc - p.endQ
  seg.l = seg.l + gapLen + c.l
  //<<Add positions to \ty{h.N}>>
  rightAnchor = true
#+end_src
#+begin_export latex
We extract sequences located between the current pair of anchors from
the subject and the query. Then we compare each nucleotide and save
the mismatches' positions to the map \ty{h.N}.
#+end_export
#+begin_src go <<Add positions to \ty{h.N}>>=
  gapSeqSubject := subject.esa.T[prevSegEnd:prevSegEnd + gapLen]
  gapSeqQuery := query.seq[p.endQ:p.endQ + gapLen]
  for i := 0; i < gapLen; i++ {
	  if gapSeqSubject[i] != gapSeqQuery[i] {
		  h.N[prevSegEnd + i] = true
	  }
  }
#+end_src
#+begin_export latex
To close the current segment is to project it onto the forward strand
(if necessary) and append it to the slice of homologies..
#+end_export
#+begin_src go <<Close current segment>>=
  if seg.s > subject.strandL {
	  seg.s = subject.totalL + 1 - seg.s - seg.l

  }
  h.S = append(h.S, seg)
#+end_src
#+begin_export latex
To open a segment is to declare its start and length, and forget that
the right anchor was found.
#+end_export
#+begin_src go <<Open a new segment>>=
  seg.s = c.startS
  seg.l = c.l
  rightAnchor = false
#+end_src
#+begin_export latex
We update the previous position in the query and the previous match.
#+end_export
#+begin_src go <<Remember current match>>=
  qp = qc
  p.l = c.l
  p.startS = c.startS
#+end_src
#+begin_export latex
\subsubsection{Find common homologous regions}
We use the set of homologous regions that we have discovered so far to
identify regions of the subject that are found in a given fraction of
the target genomes. We interpret the threshold fraction (a
\ty{Parameters} field), Section~\ref{Data structure Parameters}), then
we calculate pile heights, that is, how many times a nucleotide of the
subject is covered by piled up homologous regions. Thereafter, we
convert them to segments, and then get the actual sequences
corresponding to the segments. This task is delegated to the function
\ty{pileHeights}, which is yet to be written.
#+end_export
#+begin_src go <<Find common homologous regions>>=
  //<<Interpret the threshold fraction>>
  p := pileHeights(homologs, subject.strandL)
  //<<Convert pile heights to homologous segments>>
  //<<Convert homologs to sequences>>
#+end_src
#+begin_export latex
We calculate the product of the threshold fraction $f$
(\ty{parameters.Threshold}) and $g$, which is the total number of
genomes taken into the analysis \textit{excluding} the implied
subject. This number corresponds to the number of files in
\ty{Parameters.TargetDir} given that the directory does not contain
the reference.

We round the product down to the nearest integer, but if the
latter happens to be zero, we set the value to 1. Thus, we will be
always trying to find the partial intersection for any $0 \le f < 1$
and $g \ge 1 $.
#+end_export
#+begin_src go <<Interpret the threshold fraction>>=
  f := parameters.Threshold
  g := numFiles
  t := int(math.Floor(f * float64(g)))
  if t == 0 {
	  t = 1
  }
#+end_src
#+begin_export latex
We import \ty{math}.
#+end_export
#+begin_src go <<Imports>>=
  "math"
#+end_src
#+begin_export latex
\subsubsection{Calculate pile heights}
The function \ty{pileHeights} counts how many times a position in the
subject is covered with homologous segments found in queries.
#+end_export
#+begin_src go <<Functions>>=
  func pileHeights(h Homologs, strandL int) []int {
	  pile := make([]int, strandL)
	  for i := 0; i < len(h.S); i++ {
		  seg := h.S[i]
		  for j := seg.s; j < seg.end(); j++ {
			  pile[j] += 1
		  }
	  }
	  return pile
  }
#+end_src
#+begin_export latex
\subsubsection{Convert the pile heigths to homologous segments}
Once we have pile heights at hand, we can convert them to actual
segments of the reference. If the reference has been cleaned, two
adjacent homologous segments can be merged into one. The cleaning
implies removal of non-conventioonal nucleotides, that is, keeping
only \ty{A, T, G, C}. The removal shifts proper nucleotides that
followed the removed characters to the left. As the result, two
regions separated in the original sequence become adjacent in the
cleaned sequence (Figure~\ref{fig:MatchEnds}, A). This creates a risk
of an incorrect interpretation of pile heights unless match ends are
not neglected (Figure~\ref{fig:MatchEnds}, B).

\begin{figure}[H]
    \includegraphics[width=\linewidth]{figMatchEnds.eps}
    \caption{Normalization of the subject affects how pile heights are
      interpreted. In panel A, a region of the real SARS-CoV-2 genome
      from \ty{data/i/sars/t1.fasta} is used as a subject (reference)
      to find matches in 4 queries. This region contains 20
      undetermined nucleotides (\ty{N}), which are removed during
      normalization. The right fragment then gets a new start
      coordinate (shown in red). In panel B, the normalized sequence
      shows pile heights of 4 (grey), indicating that these
      nucleotides are present at the same positions in all 4
      queries. The red line marks the right border of the segment
      ending with \ty{TTTGT}. If the pile heights were converted to
      the final output without considering this border, they would
      produce a chimeric sequence, which is an incorrect result.}
    \label{fig:MatchEnds}
\end{figure}

To avoid chimeric sequences in the output, we create 'a map of
adjacency'. This map holds ends of the left segment from a pair. The
creation of this map is delegated to the \ty{makeMapAdj} function,
which is not written so far. Then we perform the conversion and return
the segments that make up the intersection.
#+end_export
#+begin_src go <<Convert pile heights to homologous segments>>=
  isAdj := makeMapAdj(homologs)
  intersection := pileToSeg(p, t, isAdj)
  homologs.S = intersection
#+end_src
#+begin_export latex
We define the function \ty{makeMapAdj}. We search for such pairs of
segments, where the end of the left segment is immideately followed by
the start of the right segment. In terms of 0-based coordinates, this
implies that the start and the end share the same coordinate.

We declare two auxullary variables, \ty{starts} (a map of booleans
with integer keys) and \ty{ends} (a slice of integers). Then we use
them to find ends that coincide with at least one start. We save such
ends in the adjacency map.
#+end_export
#+begin_src go <<Functions>>=
  func makeMapAdj(h Homologs) map[int]bool {
	  //<<Describe starts and ends>>
	  isAdj := make(map[int]bool)
	  for _, e := range ends {
		  if starts[e] {
			  isAdj[e] = true
		  }
	  }
	  return isAdj
  }
#+end_src
#+begin_export latex
We traverse the slice of segments, saving each segment's start to the
map \ty{starts} and each end to the slice of \ty{ends}.
#+end_export
#+begin_src go <<Describe starts and ends>>=
  starts := make(map[int]bool)
  ends := []int{}
  for i := 0; i < len(h.S); i++ {
	  seg := h.S[i]
	  starts[seg.s] = true
	  ends = append(ends, seg.end())
  }
#+end_src
#+begin_export latex
The function \ty{pileToSeg} returns a slice of segments, where each
position is covered at least a given number of times. We iterate over
keys and values of the pile.  We close an opened segment if the pile
height is below the threshold, otherwise we extend the segment. If
there is no opened segment, we open it. In the end we close the last
segment and return the result.
#+end_export
#+begin_src go <<Functions>>=
  func pileToSeg(p []int, t int, isAdj map[int]bool) []seg {
	  var segs []seg
	  var seg seg
	  segIsOpen := false
	  for k, v := range p {
		  if segIsOpen {
			  if v < t {
				  //<<Close intersection segment>>
			  } else {
				  //<<Extend intersection segment>>
			  }
		  } else {
			  //<<New intersection segment>>
		  }
	  }
	  if segIsOpen {
		  segs = append(segs, seg)
	  }
	  return segs
  }
#+end_src
#+begin_export latex
To close a segment is to append it to the resulting slice and toggle
segment's status.
#+end_export
#+begin_src go <<Close intersection segment>>=
  segs = append(segs, seg)
  segIsOpen = false
#+end_src
#+begin_export latex
The extension implies adding one to the segment's length. After the
extension we check whether the next position is listed in the
adjacency map. If so, we close the segment from here.
#+end_export
#+begin_src go <<Extend intersection segment>>=
  seg.l += 1
  if isAdj[k+1] {
	  //<<Close intersection segment>>
  }
#+end_src
#+begin_export latex
We open a new segment if the pile height is not less than the
threshold.
#+end_export
#+begin_src go <<New intersection segment>>=
  if v >= t {
	  seg.s = k
	  seg.l = 1
	  segIsOpen = true
  }
#+end_src
#+begin_export latex
\subsubsection{Convert the homologs to sequences}
We retrieve the formatting switches from the \ty{parameters} struct
and call the function \ty{homologsToFasta}, which is yet to be
written. The output of this function is the \ty{return} of
\ty{Intersect} (Section~\ref{Intersect})!
#+end_export
#+begin_src go <<Convert homologs to sequences>>=
  printN := parameters.PrintN
  printOneBased := parameters.PrintOneBased
  printSegSitePos := parameters.PrintSegSitePos
  result := homologsToFasta(homologs, subject, printN,
	  printOneBased, printSegSitePos)
  return result
#+end_src
#+begin_export latex
The function \ty{homologsToFasta()} accepts a struct of \ty{Homologs},
a struct of \ty{subject}, and \ty{bool} switches to format the output
sequence and headers. The function returns a slice of pointers to
fasta entries (type \ty{fasta.Sequence}).

We initialize the output slice, then convert each segment of the input
\ty{Homologs} into a \ty{fasta.Sequence}. For this, we construct its
data, its header, and use them to make a sequence.
#+end_export
#+begin_src go <<Functions>>=
  func homologsToFasta(h Homologs, subject subject,
	  printN bool, printOneBased bool,
	  printSegSitePos bool) []*fasta.Sequence {
	  var sequences []*fasta.Sequence
	  segs := h.S
	  ns := h.N
	  for _, seg := range segs {
		  //<<Construct sequence's data>>
		  //<<Construct sequence's header>>
		  seq := fasta.NewSequence(header, data)
		  sequences = append(sequences, seq)		
	  }
	  return sequences
  }
#+end_src
#+begin_export latex
We get start and end positions of the current segment, initialize a
slice of bytes, and append bytes to it.
#+end_export
#+begin_src go <<Construct sequence's data>>=
  start := seg.s
  end := seg.end()
  data := make([]byte, 0, seg.l)
  for j := start; j < end; j++ {
	  //<<Append next byte to the data>>
  }
#+end_src
#+begin_export latex
We decide if the next byte is going to be a nucleotide of the subject
or an \ty{N}.
#+end_export
#+begin_src go <<Append next byte to the data>>=
  if printN && ns[j] {
	  data = append(data, 'N')
  } else {
	  data = append(data, subject.esa.T[j])
  }
#+end_src
#+begin_export latex
We construct sequence's header. It shall contain the orginal contig's
header, as well as start and end coordinates on it. To find those, we
call \ty{findSegment}, which we still have to implement. We convert
zero-based end-inclusive coordinates to one-based end-exclusive
coordinates if the respective switch is set to \ty{true}. If
\ty{printSegSitePos} has been switched to \ty{true}, we build segsite
positions string with a custom function, which is yet to be
defined. Then we append the string to the header.
#+end_export
#+begin_src go <<Construct sequence's header>>=
  ch, cs, ce := findSegment(seg, subject)
  if printOneBased {
	  cs += 1
	  ce += 1
  }
  header := fmt.Sprintf("%s (%d..%d)", ch, cs, ce)
  if printSegSitePos {
	  segsites := buildSegSiteStr(seg, ns, printOneBased)
	  header += " " + segsites
  }
#+end_src
#+begin_export latex
\subsubsection{Auxillary functions for output formatting}
\textbf{\ty{findSegment}} accepts a segment and a struct of
\ty{subject}. It finds the subject's contig, to which the segment
belongs, and returns its header and coordinates of the segment on the
contig.

We initialize the output variables and 'unpack' some fields of the
\ty{subject}. Then we check if our segment is within some of the
contigs. If so, we get its header, get the segment's start and end
coordinates. A segment can be found only once, so we break as soon as
its contig is identified. We are to write the function \ty{isWithin}.
#+end_export
#+begin_src go <<Functions>>=
  func findSegment(seg seg, subject subject) (string, int, int) {
	  var ch string
	  var cs, ce int
	  contigHeaders := subject.contigHeaders
	  contigSegments := subject.contigSegments
	  for i, contigSeg := range contigSegments {
		  if isWithin(seg, contigSeg) {
			  ch = contigHeaders[i]
			  cs = seg.s - contigSeg.s
			  ce = cs + seg.l
			  break
		  }
	  }
	  return ch, cs, ce
  }
#+end_src
#+begin_export latex
\textbf{\ty{isWithin}} checks if an inner \ty{seg} is located within
an outer \ty{seg}.
#+end_export
#+begin_src go <<Functions>>=
  func isWithin(in seg, out seg) bool {
	  return in.s >= out.s && in.end() <= out.end()
  }
#+end_src
#+begin_export latex
\textbf{\ty{buildSegSiteStr}} accepts a segment, a map of
segregation sites (\ty{Homologs.N}), and a switch for printing
one-based coordinates. It builds a string of segregation site
coordinate ranges. Imagine a homologous region that has substitutions
at positions 2, 4, 6, 7, 8, 10 (6 in total). The \ty{segSiteStr}
of this sequence has the following format:
\begin{verbatim}
6; 2 4 [6:8] 10
\end{verbatim}
 We calculate the number of segregation sites and begin to build our
 \ty{segSiteStr}. Segregation sites make sense only in the
 context of homologous regions, so we add only sites found within a
 the specificed segment.
#+end_export
#+begin_src go <<Functions>>=
  func buildSegSiteStr(seg seg, ns map[int]bool,
	  printOneBased bool) string {
	  var segSiteStr string
	  if len(ns) == 0 {
		  segSiteStr = "0;"
	  } else {
		  segSiteStr += fmt.Sprintf("%d; ", len(ns))
		  //<<Add segsites found within the \ty{seg}>>
	  }
	  return segSiteStr
  }
#+end_src
#+begin_export latex
We extract the relevant positions and store them in the slice
\ty{k}. Then we scan the slice for ranges.
#+end_export
#+begin_src go <<Add segsites found within the \ty{seg}>>=
  k := []int{-1}
  //<<Extract relevant positions>>
  for i := 1; i < len(k) - 1; i++ {
	  //<<Scan for segsite ranges>>
  }
#+end_src
#+begin_export latex
We traverse \ty{ns} from the start to the end of the segment, find
existing keys, and save the coordinate in a slice. We flank the slice
of those keys with \ty{-1}.
#+end_export
#+begin_src go <<Extract relevant positions>>=
  for i := seg.s; i < seg.end(); i++ {
	  if ns[i] {
		  k = append(k, i - seg.s)
	  }
  }
  k = append(k, -1)
#+end_src
#+begin_export latex
We check if \ty{k[i]} is surrounded by preceding and consecutive
numbers. We format the output depending on the combination of possible
outcomes. If \ty{k[i]} is surrounded by such numbers, we proceed with
\ty{k[i]}. If the consecutive value exists, we open square
brackets. Should the preceding, but not the following value exist, we
close brackets. If there are no next and previous values, we print
\ty{k[i]}.  Before printing the output, we decide, which coordinate
system we use.
#+end_export
#+begin_src go <<Scan for segsite ranges>>=
  prev := k[i] == k[i-1]+1
  next := k[i] == k[i+1]-1
  if prev && next {
	  continue
  }
  //<<One- or zero-based coord?>>
  if next {
	  segSiteStr += fmt.Sprintf("[%d", coord)
  } else if prev {
	  segSiteStr += fmt.Sprintf(":%d] ", coord)
  } else {
	  segSiteStr += fmt.Sprintf("%d ", coord)
  }
#+end_src
#+begin_export latex
We check the \ty{printOneBased} switch and adjust the coordinate if
necessary.
#+end_export
#+begin_src go <<One- or zero-based coord?>>=
  coord := k[i]
  if printOneBased {
	  coord = k[i]+1
  }
#+end_src
